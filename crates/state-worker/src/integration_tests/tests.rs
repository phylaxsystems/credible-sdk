#![allow(clippy::too_many_lines)]
#![allow(clippy::manual_range_contains)]
#![allow(clippy::cast_sign_loss)]
#![allow(clippy::cast_possible_truncation)]
#![allow(clippy::cast_lossless)]
#![allow(clippy::doc_markdown)]
#![allow(clippy::cloned_ref_to_slice_refs)]
#![allow(clippy::unreadable_literal)]
#![allow(clippy::expect_fun_call)]
#![allow(clippy::uninlined_format_args)]
use crate::{
    cli::ProviderType,
    connect_provider,
    genesis,
    integration_tests::setup::LocalInstance,
    state,
    worker::StateWorker,
};
use alloy::primitives::{
    B256,
    U256,
    keccak256,
};
use anyhow::anyhow;
use async_trait::async_trait;
use int_test_utils::node_protocol_mock_server::DualProtocolMockServer;
use redis::Commands;
use serde_json::json;
use state_store::{
    ChunkedWriteConfig,
    CircularBufferConfig,
    StateReader,
    StateWriter,
    common::{
        AccountInfo,
        AccountState,
        BlockStateUpdate,
        get_account_key,
        get_storage_key,
    },
};
use std::{
    collections::HashMap,
    time::Duration,
};
use testcontainers::runners::AsyncRunner;
use testcontainers_modules::redis::Redis;
use tokio::{
    sync::broadcast,
    time::sleep,
};
use tracing_test::traced_test;

/// Helper function to get the latest block number from the circular buffer.
fn get_latest_block_from_redis(
    conn: &mut redis::Connection,
    base_namespace: &str,
    buffer_size: usize,
) -> Option<u64> {
    let mut max_block: Option<u64> = None;

    for idx in 0..buffer_size {
        let namespace = format!("{base_namespace}:{idx}");
        let block_key = format!("{namespace}:block");

        if let Ok(Some(block_str)) = conn.get::<_, Option<String>>(&block_key)
            && let Ok(block_num) = block_str.parse::<u64>()
        {
            max_block = Some(max_block.map_or(block_num, |current| current.max(block_num)));
        }
    }

    max_block
}

/// Helper to get the namespace for a given block number.
fn get_namespace_for_block(base_namespace: &str, block_number: u64, buffer_size: usize) -> String {
    let namespace_idx = usize::try_from(block_number).unwrap() % buffer_size;
    format!("{base_namespace}:{namespace_idx}")
}

#[traced_test]
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_state_worker_hydrates_genesis_state() {
    let genesis_account = "0x00000000000000000000000000000000000000aa";
    let genesis_code = "0x6000";

    let genesis_json = format!(
        r#"{{
            "alloc": {{
                "{account}": {{
                    "balance": "0x5",
                    "nonce": "0x1",
                    "code": "{code}",
                    "storage": {{
                        "0x0": "0x1",
                        "0x1": "0x2"
                    }}
                }}
            }}
        }}"#,
        account = genesis_account.trim_start_matches("0x"),
        code = genesis_code
    );
    let genesis_state =
        genesis::parse_from_str(&genesis_json).expect("failed to parse test genesis json");
    assert_eq!(
        genesis_state.accounts().len(),
        1,
        "expected single alloc entry in test genesis"
    );

    let instance = LocalInstance::new_with_setup_and_genesis(|_| {}, Some(genesis_state))
        .await
        .expect("Failed to start instance");

    sleep(Duration::from_millis(500)).await;

    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let address_hash = keccak256(hex::decode(genesis_account.trim_start_matches("0x")).unwrap());
    let namespace = "state_worker_test:0";
    let account_key = get_account_key(namespace, &address_hash.into());
    let storage_key = get_storage_key(namespace, &address_hash.into());
    let code_hash = format!(
        "0x{}",
        hex::encode(keccak256(
            hex::decode(genesis_code.trim_start_matches("0x")).unwrap()
        ))
    );
    let code_key = format!("{namespace}:code:{}", code_hash.trim_start_matches("0x"));
    let slot_zero_key = format!(
        "0x{}",
        hex::encode(keccak256(U256::ZERO.to_be_bytes::<32>()))
    );
    let slot_one_key = format!(
        "0x{}",
        hex::encode(keccak256(U256::from(1).to_be_bytes::<32>()))
    );

    let mut balance: Option<String> = None;
    let mut nonce: Option<String> = None;
    let mut stored_code: Option<String> = None;
    let mut stored_code_hash: Option<String> = None;
    let mut storage_slot_zero: Option<String> = None;
    let mut storage_slot_one: Option<String> = None;

    for _ in 0..30 {
        balance = redis::cmd("HGET")
            .arg(&account_key)
            .arg("balance")
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        nonce = redis::cmd("HGET")
            .arg(&account_key)
            .arg("nonce")
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        stored_code_hash = redis::cmd("HGET")
            .arg(&account_key)
            .arg("code_hash")
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        stored_code = redis::cmd("GET")
            .arg(&code_key)
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        storage_slot_zero = redis::cmd("HGET")
            .arg(&storage_key)
            .arg(&slot_zero_key)
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        storage_slot_one = redis::cmd("HGET")
            .arg(&storage_key)
            .arg(&slot_one_key)
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        if balance.is_some()
            && nonce.is_some()
            && stored_code.is_some()
            && stored_code_hash.is_some()
            && storage_slot_zero.is_some()
            && storage_slot_one.is_some()
        {
            break;
        }

        sleep(Duration::from_millis(50)).await;
    }

    if balance.is_none()
        || nonce.is_none()
        || stored_code.is_none()
        || stored_code_hash.is_none()
        || storage_slot_zero.is_none()
        || storage_slot_one.is_none()
    {
        let keys: Vec<String> = redis::cmd("KEYS")
            .arg("state_worker_test:*")
            .query(&mut conn)
            .unwrap_or_default();
        panic!("missing genesis data in redis. keys present: {keys:?}");
    }

    let balance = balance.unwrap();
    let nonce = nonce.unwrap();
    let stored_code = stored_code.unwrap();
    let storage_slot_zero = storage_slot_zero.unwrap();
    let storage_slot_one = storage_slot_one.unwrap();
    let stored_code_hash = stored_code_hash.unwrap();

    assert_eq!(balance, "5");
    assert_eq!(nonce, "1");
    assert_eq!(stored_code_hash.to_lowercase(), code_hash.to_lowercase());
    assert_eq!(stored_code.to_lowercase(), genesis_code.to_lowercase());
    assert_eq!(
        storage_slot_zero,
        format!("0x{}", hex::encode(B256::from(U256::from(1))))
    );
    assert_eq!(
        storage_slot_one,
        format!("0x{}", hex::encode(B256::from(U256::from(2))))
    );

    let block_key = format!("{namespace}:block");
    let current_block: String = conn
        .get(&block_key)
        .expect("Failed to read current block from namespace 0");
    assert_eq!(current_block, "0");
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_state_worker_processes_multiple_state_changes() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    sleep(Duration::from_millis(200)).await;

    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    let parity_trace_block_2 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x2222222222222222222222222222222222222222222222222222222222222222",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    let parity_trace_block_3 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x3333333333333333333333333333333333333333333333333333333333333333",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(200)).await;

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_2);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(200)).await;

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_3);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(200)).await;

    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let mut current_block: Option<u64> = None;
    for _ in 0..30 {
        current_block = get_latest_block_from_redis(&mut conn, "state_worker_test", 3);
        if current_block == Some(3) {
            break;
        }
        sleep(Duration::from_millis(100)).await;
    }

    assert_eq!(
        current_block,
        Some(3),
        "Expected current block to be 3, got {current_block:?}",
    );

    let expected_block_hashes = vec![
        (
            "state_worker_test:block_hash:1",
            "0x0000000000000000000000000000000000000000000000000000000000000001",
        ),
        (
            "state_worker_test:block_hash:2",
            "0x0000000000000000000000000000000000000000000000000000000000000002",
        ),
        (
            "state_worker_test:block_hash:3",
            "0x0000000000000000000000000000000000000000000000000000000000000003",
        ),
    ];

    for (key, expected_hash) in &expected_block_hashes {
        let mut actual_hash: Option<String> = None;
        for _ in 0..20 {
            if let Ok(hash) = conn.get::<_, String>(key) {
                actual_hash = Some(hash);
                break;
            }
            sleep(Duration::from_millis(50)).await;
        }

        let actual_hash = actual_hash.unwrap_or_else(|| panic!("Failed to get {key} from Redis"));
        assert_eq!(
            &actual_hash, expected_hash,
            "Expected block hash {expected_hash} for key {key}, got {actual_hash}",
        );
    }

    let namespace_1 = "state_worker_test:1";
    let block_1_key = format!("{namespace_1}:block");
    let mut block_1: Option<String> = None;
    for _ in 0..30 {
        if let Ok(b) = conn.get::<_, String>(&block_1_key) {
            block_1 = Some(b);
            break;
        }
        sleep(Duration::from_millis(100)).await;
    }
    let block_1 = block_1.expect("Failed to get block 1 from namespace 1");
    assert_eq!(block_1, "1");

    let namespace_2 = "state_worker_test:2";
    let block_2_key = format!("{namespace_2}:block");
    let mut block_2: Option<String> = None;
    for _ in 0..20 {
        if let Ok(b) = conn.get::<_, String>(&block_2_key) {
            block_2 = Some(b);
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }
    let block_2 = block_2.expect("Failed to get block 2 from namespace 2");
    assert_eq!(block_2, "2");

    let namespace_0 = "state_worker_test:0";
    let block_3_key = format!("{namespace_0}:block");
    let mut block_3: Option<String> = None;
    for _ in 0..20 {
        if let Ok(b) = conn.get::<_, String>(&block_3_key) {
            block_3 = Some(b);
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }
    let block_3 = block_3.expect("Failed to get block 3 from namespace 0");
    assert_eq!(block_3, "3");

    for (key, expected_hash) in [
        (
            "state_worker_test:block_hash:1",
            "0x0000000000000000000000000000000000000000000000000000000000000001",
        ),
        (
            "state_worker_test:block_hash:2",
            "0x0000000000000000000000000000000000000000000000000000000000000002",
        ),
        (
            "state_worker_test:block_hash:3",
            "0x0000000000000000000000000000000000000000000000000000000000000003",
        ),
    ] {
        let hash: String = conn
            .get(key)
            .unwrap_or_else(|_| panic!("Failed to get {key}"));
        assert_eq!(hash, expected_hash);

        let hex_part = &hash[2..];
        assert!(
            hex_part.chars().all(|c| c.is_ascii_hexdigit()),
            "Block hash should contain only hex digits: {hash}",
        );
    }
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_state_worker_handles_rapid_state_changes() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    for i in 0..5 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [
                {
                    "transactionHash": format!("0x{:064x}", i + 1),
                    "trace": [],
                    "vmTrace": null,
                    "stateDiff": {},
                    "output": "0x"
                }
            ]
        });

        instance
            .http_server_mock
            .add_response("trace_replayBlockTransactions", parity_trace);
        instance.http_server_mock.send_new_head();
        sleep(Duration::from_millis(100)).await;
    }

    sleep(Duration::from_millis(200)).await;

    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let mut current_block: Option<u64> = None;
    for _ in 0..30 {
        current_block = get_latest_block_from_redis(&mut conn, "state_worker_test", 3);
        if current_block == Some(5) {
            break;
        }
        sleep(Duration::from_millis(100)).await;
    }

    let current_block_num = current_block.expect("Failed to get current block after timeout");

    assert_eq!(
        current_block_num, 5,
        "Expected 5 blocks to be processed, got {current_block_num}",
    );

    for block_num in 1..=current_block_num {
        let key = format!("state_worker_test:block_hash:{block_num}");
        let mut hash: Option<String> = None;
        for _ in 0..20 {
            if let Ok(h) = conn.get::<_, String>(&key) {
                hash = Some(h);
                break;
            }
            sleep(Duration::from_millis(50)).await;
        }
        let hash = hash.unwrap_or_else(|| panic!("Failed to get block hash for block {block_num}"));
        assert!(hash.starts_with("0x"), "Block hash should start with 0x");
        assert_eq!(hash.len(), 66, "Block hash should be 66 characters long");
    }

    for block_num in 3..=5 {
        let namespace = get_namespace_for_block("state_worker_test", block_num, 3);
        let block_key = format!("{namespace}:block");

        let mut stored_block: Option<String> = None;
        for _ in 0..20 {
            if let Ok(b) = conn.get::<_, String>(&block_key) {
                stored_block = Some(b);
                break;
            }
            sleep(Duration::from_millis(50)).await;
        }

        let stored_block = stored_block
            .unwrap_or_else(|| panic!("Failed to get block {block_num} from {namespace}"));
        assert_eq!(
            stored_block,
            block_num.to_string(),
            "Namespace should contain block {block_num}"
        );
    }
}

#[traced_test]
#[tokio::test]
async fn test_state_worker_non_consecutive_blocks_critical_alert() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    // Setup valid trace response for block 1 with empty stateDiff
    let valid_parity_trace_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", valid_parity_trace_1);
    instance.http_server_mock.send_new_head_with_block_number(1);
    sleep(Duration::from_millis(50)).await;

    // Mock an error for block 2 to simulate it not existing
    instance.http_server_mock.mock_rpc_error(
        "trace_replayBlockTransactions",
        -32000,
        "block not found",
    );
    instance.http_server_mock.send_new_head_with_block_number(2);
    sleep(Duration::from_millis(50)).await;

    // Setup valid trace response for block 3 with empty stateDiff
    let valid_parity_trace_3 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x3333333333333333333333333333333333333333333333333333333333333333",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", valid_parity_trace_3);
    instance.http_server_mock.send_new_head_with_block_number(3);
    sleep(Duration::from_millis(50)).await;

    assert!(logs_contain("critical"));
}

#[traced_test]
#[tokio::test]
async fn test_state_worker_missing_state_critical_alert() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head_with_block_number(1);
    sleep(Duration::from_millis(50)).await;

    let parity_trace_block_3 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x3333333333333333333333333333333333333333333333333333333333333333",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_3);
    instance.http_server_mock.send_new_head_with_block_number(3);
    sleep(Duration::from_millis(50)).await;

    assert!(logs_contain("critical"));
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_circular_buffer_rotation_with_state_diffs() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    for i in 0..6 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [
                {
                    "transactionHash": format!("0x{:064x}", i + 1),
                    "trace": [],
                    "vmTrace": null,
                    "stateDiff": {},
                    "output": "0x"
                }
            ]
        });

        instance
            .http_server_mock
            .add_response("trace_replayBlockTransactions", parity_trace);
        instance.http_server_mock.send_new_head();
        sleep(Duration::from_millis(100)).await;
    }

    sleep(Duration::from_millis(300)).await;

    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let mut latest: Option<u64> = None;
    for _ in 0..30 {
        latest = get_latest_block_from_redis(&mut conn, "state_worker_test", 3);
        if latest == Some(6) {
            break;
        }
        sleep(Duration::from_millis(100)).await;
    }
    assert_eq!(latest, Some(6), "Latest block should be 6 after rotation");

    for block_num in 4..=6 {
        let namespace = get_namespace_for_block("state_worker_test", block_num, 3);
        let block_key = format!("{namespace}:block");

        let mut stored_block: Option<String> = None;
        for _ in 0..20 {
            if let Ok(b) = conn.get::<_, String>(&block_key) {
                stored_block = Some(b);
                break;
            }
            sleep(Duration::from_millis(50)).await;
        }

        let stored_block = stored_block
            .unwrap_or_else(|| panic!("Failed to get block {block_num} from {namespace}"));
        assert_eq!(
            stored_block,
            block_num.to_string(),
            "Namespace should contain block {block_num}"
        );
    }

    for block_num in 1..=6 {
        let key = format!("state_worker_test:block_hash:{block_num}");
        let mut hash: Option<String> = None;
        for _ in 0..20 {
            if let Ok(h) = conn.get::<_, String>(&key) {
                hash = Some(h);
                break;
            }
            sleep(Duration::from_millis(50)).await;
        }
        let hash = hash.unwrap_or_else(|| panic!("Block hash for block {block_num} should exist"));
        assert!(hash.starts_with("0x"));
    }

    for block_num in 4..=6 {
        let diff_key = format!("state_worker_test:diff:{block_num}");
        let mut exists = false;
        for _ in 0..20 {
            if let Ok(e) = redis::cmd("EXISTS").arg(&diff_key).query::<bool>(&mut conn) {
                exists = e;
                if exists {
                    break;
                }
            }
            sleep(Duration::from_millis(50)).await;
        }
        assert!(exists, "State diff for block {block_num} should exist");
    }

    sleep(Duration::from_millis(100)).await;

    for block_num in 1..=3 {
        let diff_key = format!("state_worker_test:diff:{block_num}");
        let exists: bool = redis::cmd("EXISTS")
            .arg(&diff_key)
            .query(&mut conn)
            .unwrap_or(true);
        assert!(
            !exists,
            "Old state diff for block {block_num} should be deleted"
        );
    }
}

/// Test basic restart: process blocks, stop, restart, verify continuation
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_restart_continues_from_last_block() {
    // Start Redis container
    let redis_container = Redis::default()
        .start()
        .await
        .expect("Failed to start Redis container");

    let host = redis_container
        .get_host()
        .await
        .expect("Failed to get Redis host");
    let port = redis_container
        .get_host_port_ipv4(6379)
        .await
        .expect("Failed to get Redis port");
    let redis_url = format!("redis://{host}:{port}");

    // Setup mock server
    let http_server_mock = DualProtocolMockServer::new()
        .await
        .expect("Failed to create mock server");

    // Setup traces for blocks 1-3
    for i in 1..=3 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [{
                "transactionHash": format!("0x{:064x}", i),
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }]
        });
        http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
    }

    // First run: process blocks 1-3
    {
        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "restart_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "restart_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer.clone(), reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        // Start worker in background
        let worker_handle = tokio::spawn(async move { worker.run(Some(0), shutdown_rx).await });

        // Send 3 blocks
        for _ in 0..3 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        // Wait for blocks to be processed
        sleep(Duration::from_millis(300)).await;

        // Verify blocks 1-3 are processed
        let client = redis::Client::open(redis_url.as_str()).unwrap();
        let mut conn = client.get_connection().unwrap();
        let latest = get_latest_block_from_redis(&mut conn, "restart_test", 3);
        assert_eq!(latest, Some(3), "Should have processed 3 blocks");

        // Shutdown first worker
        shutdown_tx.send(()).unwrap();
        let _ = tokio::time::timeout(Duration::from_secs(2), worker_handle).await;
    }

    // Setup traces for blocks 4-6
    for i in 4..=6 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [{
                "transactionHash": format!("0x{:064x}", i),
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }]
        });
        http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
    }

    // Second run: restart and process blocks 4-6
    {
        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "restart_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "restart_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        // Verify it starts from the correct block
        let start_block = writer.latest_block_number().unwrap();
        assert_eq!(start_block, Some(3), "Should resume from block 3");

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer.clone(), reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        let worker_handle = tokio::spawn(async move {
            // Pass None to use automatic detection
            worker.run(None, shutdown_rx).await
        });

        // Send blocks 4-6
        for _ in 0..3 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        sleep(Duration::from_millis(300)).await;

        // Verify all blocks are processed
        let client = redis::Client::open(redis_url.as_str()).unwrap();
        let mut conn = client.get_connection().unwrap();
        let latest = get_latest_block_from_redis(&mut conn, "restart_test", 3);
        assert_eq!(
            latest,
            Some(6),
            "Should have processed blocks 4-6 after restart"
        );

        // Verify block hashes for all blocks exist
        for block_num in 1..=6 {
            let key = format!("restart_test:block_hash:{block_num}");
            let hash: Option<String> = conn.get(&key).ok();
            assert!(
                hash.is_some(),
                "Block hash for block {block_num} should exist"
            );
        }

        shutdown_tx.send(()).unwrap();
        let _ = tokio::time::timeout(Duration::from_secs(2), worker_handle).await;
    }
}

/// Test restart with circular buffer wrap: verify diffs are correctly applied
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_restart_with_buffer_wrap_applies_diffs() {
    let redis_container = Redis::default()
        .start()
        .await
        .expect("Failed to start Redis container");

    let host = redis_container
        .get_host()
        .await
        .expect("Failed to get Redis host");
    let port = redis_container
        .get_host_port_ipv4(6379)
        .await
        .expect("Failed to get Redis port");
    let redis_url = format!("redis://{host}:{port}");

    let http_server_mock = DualProtocolMockServer::new()
        .await
        .expect("Failed to create mock server");

    // First run: process blocks to wrap the buffer (blocks 0-4)
    {
        for i in 0..=4 {
            let parity_trace = json!({
                "jsonrpc": "2.0",
                "id": 1,
                "result": [{
                    "transactionHash": format!("0x{:064x}", i),
                    "trace": [],
                    "vmTrace": null,
                    "stateDiff": {},
                    "output": "0x"
                }]
            });
            http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
        }

        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "wrap_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "wrap_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer, reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        let worker_handle = tokio::spawn(async move { worker.run(Some(0), shutdown_rx).await });

        for _ in 0..4 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        sleep(Duration::from_millis(300)).await;

        let client = redis::Client::open(redis_url.as_str()).unwrap();
        let mut conn = client.get_connection().unwrap();
        let latest = get_latest_block_from_redis(&mut conn, "wrap_test", 3);
        assert_eq!(latest, Some(4), "Should have processed blocks 0-4");

        // Verify namespace:1 contains block 4 (wrapped from block 1)
        let namespace_1 = "wrap_test:1";
        let block_key = format!("{namespace_1}:block");
        let block: String = conn.get(&block_key).unwrap();
        assert_eq!(block, "4", "Namespace 1 should contain block 4 after wrap");

        shutdown_tx.send(()).unwrap();
        let _ = tokio::time::timeout(Duration::from_secs(2), worker_handle).await;
    }

    // Second run: restart and verify diffs are still available for reconstruction
    {
        // Add traces for blocks 5-7
        for i in 5..=7 {
            let parity_trace = json!({
                "jsonrpc": "2.0",
                "id": 1,
                "result": [{
                    "transactionHash": format!("0x{:064x}", i),
                    "trace": [],
                    "vmTrace": null,
                    "stateDiff": {},
                    "output": "0x"
                }]
            });
            http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
        }

        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "wrap_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "wrap_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer, reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        let worker_handle = tokio::spawn(async move { worker.run(None, shutdown_rx).await });

        for _ in 0..3 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        sleep(Duration::from_millis(300)).await;

        let client = redis::Client::open(redis_url.as_str()).unwrap();
        let mut conn = client.get_connection().unwrap();
        let latest = get_latest_block_from_redis(&mut conn, "wrap_test", 3);
        assert_eq!(latest, Some(7), "Should continue to block 7 after restart");

        // Verify diffs exist for recent blocks
        for block_num in 5..=7 {
            let diff_key = format!("wrap_test:diff:{block_num}");
            let exists: bool = redis::cmd("EXISTS")
                .arg(&diff_key)
                .query(&mut conn)
                .unwrap();
            assert!(exists, "Diff for block {block_num} should exist");
        }

        shutdown_tx.send(()).unwrap();
        let _ = tokio::time::timeout(Duration::from_secs(2), worker_handle).await;
    }
}

/// Test restart after crash during block processing
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_restart_after_mid_block_crash() {
    let redis_container = Redis::default()
        .start()
        .await
        .expect("Failed to start Redis container");

    let host = redis_container
        .get_host()
        .await
        .expect("Failed to get Redis host");
    let port = redis_container
        .get_host_port_ipv4(6379)
        .await
        .expect("Failed to get Redis port");
    let redis_url = format!("redis://{host}:{port}");

    let http_server_mock = DualProtocolMockServer::new()
        .await
        .expect("Failed to create mock server");

    // Process blocks 0-2 successfully
    for i in 0..=2 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [{
                "transactionHash": format!("0x{:064x}", i),
                "trace": [],
                "vmTrace": null,
                "stateDiff": {},
                "output": "0x"
            }]
        });
        http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
    }

    {
        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "crash_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "crash_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer, reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        let worker_handle = tokio::spawn(async move { worker.run(Some(0), shutdown_rx).await });

        for _ in 0..2 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        sleep(Duration::from_millis(200)).await;

        // Simulate crash - abrupt shutdown
        drop(shutdown_tx);
        drop(worker_handle);
    }

    // Restart: should continue from block 3
    {
        for i in 3..=5 {
            let parity_trace = json!({
                "jsonrpc": "2.0",
                "id": 1,
                "result": [{
                    "transactionHash": format!("0x{:064x}", i),
                    "trace": [],
                    "vmTrace": null,
                    "stateDiff": {},
                    "output": "0x"
                }]
            });
            http_server_mock.add_response("trace_replayBlockTransactions", parity_trace);
        }

        let provider = connect_provider(&http_server_mock.ws_url())
            .await
            .expect("Failed to connect to provider");

        let writer = StateWriter::new(
            &redis_url,
            "crash_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        let reader = StateReader::new(
            &redis_url,
            "crash_test",
            CircularBufferConfig::new(3).unwrap(),
        )
        .unwrap();

        // Verify it detects the correct resume point
        let start_block = writer.latest_block_number().unwrap();
        assert_eq!(start_block, Some(2), "Should detect last completed block");

        let trace_provider = state::create_trace_provider(
            ProviderType::Parity,
            provider.clone(),
            Duration::from_secs(30),
        );

        let mut worker = StateWorker::new(provider, trace_provider, writer, reader, None);
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        let worker_handle = tokio::spawn(async move { worker.run(None, shutdown_rx).await });

        for _ in 0..3 {
            http_server_mock.send_new_head();
            sleep(Duration::from_millis(100)).await;
        }

        sleep(Duration::from_millis(300)).await;

        let client = redis::Client::open(redis_url.as_str()).unwrap();
        let mut conn = client.get_connection().unwrap();
        let latest = get_latest_block_from_redis(&mut conn, "crash_test", 3);
        assert_eq!(latest, Some(5), "Should recover and process to block 5");

        shutdown_tx.send(()).unwrap();
        let _ = tokio::time::timeout(Duration::from_secs(2), worker_handle).await;
    }
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_zero_storage_values_are_deleted() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let test_address = "0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa";
    let address_hash = keccak256(hex::decode(test_address.trim_start_matches("0x")).unwrap());

    // Block 1: Set storage slots 0 and 1 to non-zero values
    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": { "+": "0x100" },
                        "nonce": { "+": "0x1" },
                        "code": { "+": "0x" },
                        "storage": {
                            "0x0000000000000000000000000000000000000000000000000000000000000000": { "+": "0x0000000000000000000000000000000000000000000000000000000000000001" },
                            "0x0000000000000000000000000000000000000000000000000000000000000001": { "+": "0x0000000000000000000000000000000000000000000000000000000000000002" }
                        }
                    }
                },
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(200)).await;

    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let namespace = get_namespace_for_block("state_worker_test", 1, 3);
    let storage_key = get_storage_key(&namespace, &address_hash.into());

    let slot_zero_key = format!(
        "0x{}",
        hex::encode(keccak256(U256::ZERO.to_be_bytes::<32>()))
    );
    let slot_one_key = format!(
        "0x{}",
        hex::encode(keccak256(U256::from(1).to_be_bytes::<32>()))
    );

    // Wait for block 1 to be processed and verify both slots exist
    let mut slot_zero: Option<String> = None;
    let mut slot_one: Option<String> = None;
    for _ in 0..30 {
        slot_zero = redis::cmd("HGET")
            .arg(&storage_key)
            .arg(&slot_zero_key)
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();
        slot_one = redis::cmd("HGET")
            .arg(&storage_key)
            .arg(&slot_one_key)
            .query::<Option<String>>(&mut conn)
            .ok()
            .flatten();

        if slot_zero.is_some() && slot_one.is_some() {
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }

    assert!(slot_zero.is_some(), "Slot 0 should exist after block 1");
    assert!(slot_one.is_some(), "Slot 1 should exist after block 1");

    // Block 2: Set slot 1 to zero (should be deleted)
    let parity_trace_block_2 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x2222222222222222222222222222222222222222222222222222222222222222",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": "=",
                        "nonce": "=",
                        "code": "=",
                        "storage": {
                            "0x0000000000000000000000000000000000000000000000000000000000000001": {
                                "*": {
                                    "from": "0x0000000000000000000000000000000000000000000000000000000000000002",
                                    "to": "0x0000000000000000000000000000000000000000000000000000000000000000"
                                }
                            }
                        }
                    }
                },
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_2);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(200)).await;

    // Wait for block 2 to be processed
    let namespace_2 = get_namespace_for_block("state_worker_test", 2, 3);
    let block_2_key = format!("{namespace_2}:block");
    for _ in 0..30 {
        if let Ok(b) = conn.get::<_, String>(&block_2_key)
            && b == "2"
        {
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }

    // Verify slot 0 still exists and slot 1 was deleted
    let storage_key_2 = get_storage_key(&namespace_2, &address_hash.into());

    let slot_zero_after: Option<String> = redis::cmd("HGET")
        .arg(&storage_key_2)
        .arg(&slot_zero_key)
        .query(&mut conn)
        .ok()
        .flatten();

    let slot_one_after: Option<String> = redis::cmd("HGET")
        .arg(&storage_key_2)
        .arg(&slot_one_key)
        .query(&mut conn)
        .ok()
        .flatten();

    assert!(
        slot_zero_after.is_some(),
        "Slot 0 should still exist after block 2"
    );
    assert!(
        slot_one_after.is_none(),
        "Slot 1 should be deleted after being set to zero in block 2"
    );
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_get_account_does_not_load_storage() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let test_address = "0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa";
    let address_hash = keccak256(hex::decode(test_address.trim_start_matches("0x")).unwrap());

    // Block 1: Create account with multiple storage slots
    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": { "+": "0x100" },
                        "nonce": { "+": "0x1" },
                        "code": { "+": "0x6080" },
                        "storage": {
                            "0x0000000000000000000000000000000000000000000000000000000000000000": { "+": "0x0000000000000000000000000000000000000000000000000000000000000001" },
                            "0x0000000000000000000000000000000000000000000000000000000000000001": { "+": "0x0000000000000000000000000000000000000000000000000000000000000002" },
                            "0x0000000000000000000000000000000000000000000000000000000000000002": { "+": "0x0000000000000000000000000000000000000000000000000000000000000003" }
                        }
                    }
                },
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(300)).await;

    // Wait for block to be processed
    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let namespace = get_namespace_for_block("state_worker_test", 1, 3);
    let block_key = format!("{namespace}:block");

    for _ in 0..30 {
        if let Ok(b) = conn.get::<_, String>(&block_key)
            && b == "1"
        {
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }

    // Create reader
    let reader = StateReader::new(
        &instance.redis_url,
        "state_worker_test",
        CircularBufferConfig::new(3).unwrap(),
    )
    .expect("Failed to create reader");

    // Test get_account returns AccountInfo (without storage field at all)
    let account_info: AccountInfo = reader
        .get_account(address_hash.into(), 1)
        .expect("Failed to get account")
        .expect("Account should exist");

    assert_eq!(account_info.balance, U256::from(0x100));
    assert_eq!(account_info.nonce, 1);

    // Test get_full_account returns AccountState (with storage)
    let account_state: AccountState = reader
        .get_full_account(address_hash.into(), 1)
        .expect("Failed to get account with storage")
        .expect("Account should exist");

    assert_eq!(account_state.balance, U256::from(0x100));
    assert_eq!(account_state.nonce, 1);
    assert_eq!(
        account_state.storage.len(),
        3,
        "get_full_account should return all storage slots"
    );
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_get_full_account_returns_all_slots() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let test_address = "0xbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb";
    let address_hash = keccak256(hex::decode(test_address.trim_start_matches("0x")).unwrap());

    // Block 1: Create account with storage
    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": { "+": "0x200" },
                        "nonce": { "+": "0x5" },
                        "code": { "+": "0x" },
                        "storage": {
                            "0x0000000000000000000000000000000000000000000000000000000000000000": { "+": "0x000000000000000000000000000000000000000000000000000000000000000a" },
                            "0x0000000000000000000000000000000000000000000000000000000000000001": { "+": "0x000000000000000000000000000000000000000000000000000000000000000b" }
                        }
                    }
                },
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(300)).await;

    // Wait for block to be processed
    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let namespace = get_namespace_for_block("state_worker_test", 1, 3);
    let block_key = format!("{namespace}:block");

    for _ in 0..30 {
        if let Ok(b) = conn.get::<_, String>(&block_key)
            && b == "1"
        {
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }

    let reader = StateReader::new(
        &instance.redis_url,
        "state_worker_test",
        CircularBufferConfig::new(3).unwrap(),
    )
    .expect("Failed to create reader");

    // Test get_full_account returns AccountState
    let account: AccountState = reader
        .get_full_account(address_hash.into(), 1)
        .expect("Failed to get account with storage")
        .expect("Account should exist");

    assert_eq!(account.balance, U256::from(0x200));
    assert_eq!(account.nonce, 5);
    assert_eq!(account.storage.len(), 2);

    // Verify storage values
    let slot_0_hash = keccak256(U256::ZERO.to_be_bytes::<32>());
    let slot_1_hash = keccak256(U256::from(1).to_be_bytes::<32>());

    let slot_0_key = U256::from_be_bytes(slot_0_hash.into());
    let slot_1_key = U256::from_be_bytes(slot_1_hash.into());

    assert!(
        account.storage.contains_key(&slot_0_key),
        "Storage should contain slot 0"
    );
    assert!(
        account.storage.contains_key(&slot_1_key),
        "Storage should contain slot 1"
    );
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_get_storage_returns_individual_slot() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let test_address = "0xcccccccccccccccccccccccccccccccccccccccc";
    let address_hash = keccak256(hex::decode(test_address.trim_start_matches("0x")).unwrap());

    // Block 1: Create account with storage
    let parity_trace_block_1 = json!({
        "jsonrpc": "2.0",
        "id": 1,
        "result": [
            {
                "transactionHash": "0x1111111111111111111111111111111111111111111111111111111111111111",
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": { "+": "0x300" },
                        "nonce": { "+": "0x1" },
                        "code": { "+": "0x" },
                        "storage": {
                            "0x0000000000000000000000000000000000000000000000000000000000000005": { "+": "0x00000000000000000000000000000000000000000000000000000000000000ff" }
                        }
                    }
                },
                "output": "0x"
            }
        ]
    });

    instance
        .http_server_mock
        .add_response("trace_replayBlockTransactions", parity_trace_block_1);
    instance.http_server_mock.send_new_head();
    sleep(Duration::from_millis(300)).await;

    // Wait for block to be processed
    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let namespace = get_namespace_for_block("state_worker_test", 1, 3);
    let block_key = format!("{namespace}:block");

    for _ in 0..30 {
        if let Ok(b) = conn.get::<_, String>(&block_key)
            && b == "1"
        {
            break;
        }
        sleep(Duration::from_millis(50)).await;
    }

    let reader = StateReader::new(
        &instance.redis_url,
        "state_worker_test",
        CircularBufferConfig::new(3).unwrap(),
    )
    .expect("Failed to create reader");

    // Use get_storage for individual slot lookup
    let slot_5_hash = keccak256(U256::from(5).to_be_bytes::<32>());
    let slot_key = U256::from_be_bytes(slot_5_hash.into());

    let value = reader
        .get_storage(address_hash.into(), slot_key, 1)
        .expect("Failed to get storage")
        .expect("Storage slot should exist");

    assert_eq!(value, U256::from(0xff));

    // Verify non-existent slot returns None
    let non_existent_slot = keccak256(U256::from(999).to_be_bytes::<32>());
    let non_existent_key = U256::from_be_bytes(non_existent_slot.into());

    let missing = reader
        .get_storage(address_hash.into(), non_existent_key, 1)
        .expect("Failed to get storage");

    assert!(missing.is_none(), "Non-existent slot should return None");
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_eip2935_and_eip4788_system_contracts() {
    use crate::system_calls::HISTORY_BUFFER_LENGTH;
    use alloy::eips::{
        eip2935::{
            HISTORY_STORAGE_ADDRESS,
            HISTORY_STORAGE_CODE,
        },
        eip4788::{
            BEACON_ROOTS_ADDRESS,
            BEACON_ROOTS_CODE,
        },
    };

    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");

    let test_address = "0xdddddddddddddddddddddddddddddddddddddddd";

    // Process 3 blocks with a regular transaction to verify system calls merge with trace state
    for i in 1..=3 {
        let parity_trace = json!({
            "jsonrpc": "2.0",
            "id": 1,
            "result": [{
                "transactionHash": format!("0x{:064x}", i),
                "trace": [],
                "vmTrace": null,
                "stateDiff": {
                    test_address: {
                        "balance": { "+": format!("0x{:x}", i * 0x100) },
                        "nonce": { "+": format!("0x{:x}", i) },
                        "code": { "+": "0x" },
                        "storage": {}
                    }
                },
                "output": "0x"
            }]
        });
        instance
            .http_server_mock
            .add_response("trace_replayBlockTransactions", parity_trace);
        instance.http_server_mock.send_new_head();
        sleep(Duration::from_millis(150)).await;
    }

    sleep(Duration::from_millis(300)).await;

    // Wait for block 3 to be processed
    let client =
        redis::Client::open(instance.redis_url.as_str()).expect("Failed to create Redis client");
    let mut conn = client
        .get_connection()
        .expect("Failed to get Redis connection");

    let mut latest: Option<u64> = None;
    for _ in 0..30 {
        latest = get_latest_block_from_redis(&mut conn, "state_worker_test", 3);
        if latest == Some(3) {
            break;
        }
        sleep(Duration::from_millis(100)).await;
    }
    assert_eq!(latest, Some(3), "Should process 3 blocks");

    let reader = StateReader::new(
        &instance.redis_url,
        "state_worker_test",
        CircularBufferConfig::new(3).unwrap(),
    )
    .expect("Failed to create reader");

    // Verify the regular account from trace exists (system calls merge with trace state)
    let test_address_hash = keccak256(hex::decode(test_address.trim_start_matches("0x")).unwrap());
    let test_account = reader
        .get_account(test_address_hash.into(), 3)
        .expect("Failed to get test account")
        .expect("Test account should exist");
    assert_eq!(test_account.balance, U256::from(0x300));

    // EIP-2935 Verification
    let eip2935_address_hash = keccak256(HISTORY_STORAGE_ADDRESS);
    let eip2935_account = reader
        .get_full_account(eip2935_address_hash.into(), 3)
        .expect("Failed to get EIP-2935 account")
        .expect("EIP-2935 account should exist");

    // Verify code hash
    let expected_eip2935_code_hash = keccak256(&HISTORY_STORAGE_CODE);
    assert_eq!(
        eip2935_account.code_hash, expected_eip2935_code_hash,
        "EIP-2935 should have correct code hash"
    );
    assert_eq!(eip2935_account.nonce, 1, "EIP-2935 nonce should be 1");
    assert_eq!(
        eip2935_account.balance,
        U256::ZERO,
        "EIP-2935 balance should be 0"
    );

    // Verify storage contains parent block hashes (slot = block_number % 8191)
    assert!(
        !eip2935_account.storage.is_empty(),
        "EIP-2935 should have storage slots for parent hashes"
    );

    // EIP-4788 Verification
    let eip4788_address_hash = keccak256(BEACON_ROOTS_ADDRESS);
    let eip4788_account = reader
        .get_full_account(eip4788_address_hash.into(), 3)
        .expect("Failed to get EIP-4788 account")
        .expect("EIP-4788 account should exist");

    // Verify code hash
    let expected_eip4788_code_hash = keccak256(&BEACON_ROOTS_CODE);
    assert_eq!(
        eip4788_account.code_hash, expected_eip4788_code_hash,
        "EIP-4788 should have correct code hash"
    );
    assert_eq!(eip4788_account.nonce, 1, "EIP-4788 nonce should be 1");
    assert_eq!(
        eip4788_account.balance,
        U256::ZERO,
        "EIP-4788 balance should be 0"
    );

    // Verify the dual ring buffer pattern: timestamp at slot N, root at slot N + 8191
    assert!(
        eip4788_account.storage.len() >= 2,
        "EIP-4788 should have at least 2 storage slots (timestamp + root)"
    );

    let mut has_low_slot = false;
    let mut has_high_slot = false;
    for slot in eip4788_account.storage.keys() {
        let slot_u64 = slot.to::<u64>();
        if slot_u64 < HISTORY_BUFFER_LENGTH {
            has_low_slot = true;
        } else if slot_u64 >= HISTORY_BUFFER_LENGTH && slot_u64 < 2 * HISTORY_BUFFER_LENGTH {
            has_high_slot = true;
        }
    }
    assert!(
        has_low_slot,
        "EIP-4788 should have timestamp slot (< HISTORY_BUFFER_LENGTH)"
    );
    assert!(
        has_high_slot,
        "EIP-4788 should have root slot (>= HISTORY_BUFFER_LENGTH)"
    );
}

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_genesis_block_skips_system_calls() {
    use alloy::eips::{
        eip2935::HISTORY_STORAGE_ADDRESS,
        eip4788::BEACON_ROOTS_ADDRESS,
    };

    // Create genesis with a test account
    let genesis_json = r#"{
        "alloc": {
            "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa": {
                "balance": "0x100",
                "nonce": "0x0"
            }
        }
    }"#;

    let genesis_state =
        genesis::parse_from_str(genesis_json).expect("failed to parse test genesis json");

    let instance = LocalInstance::new_with_setup_and_genesis(|_| {}, Some(genesis_state))
        .await
        .expect("Failed to start instance");

    sleep(Duration::from_millis(500)).await;

    let reader = StateReader::new(
        &instance.redis_url,
        "state_worker_test",
        CircularBufferConfig::new(3).unwrap(),
    )
    .expect("Failed to create reader");

    // Verify genesis account exists
    let genesis_address_hash =
        keccak256(hex::decode("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa").unwrap());
    let genesis_account = reader
        .get_account(genesis_address_hash.into(), 0)
        .expect("Failed to get genesis account");
    assert!(
        genesis_account.is_some(),
        "Genesis account should exist at block 0"
    );

    // Verify system contracts do NOT exist at block 0 (system calls skip genesis)
    let eip2935_address_hash = keccak256(HISTORY_STORAGE_ADDRESS);
    let eip2935_account = reader
        .get_account(eip2935_address_hash.into(), 0)
        .expect("Failed to get EIP-2935 account");
    assert!(
        eip2935_account.is_none(),
        "EIP-2935 should NOT exist at genesis block (block 0)"
    );

    let eip4788_address_hash = keccak256(BEACON_ROOTS_ADDRESS);
    let eip4788_account = reader
        .get_account(eip4788_address_hash.into(), 0)
        .expect("Failed to get EIP-4788 account");
    assert!(
        eip4788_account.is_none(),
        "EIP-4788 should NOT exist at genesis block (block 0)"
    );
}

#[tokio::test]
async fn test_recover_stale_locks_empty_when_no_locks() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_no_locks_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let writer = StateWriter::new(&instance.redis_url, &base_namespace, config).unwrap();

    let recoveries = writer.recover_stale_locks().unwrap();
    assert!(recoveries.is_empty(), "expected no stale locks");
}

#[tokio::test]
async fn test_recover_stale_locks_clears_completed_write() {
    // Test case: crash happened AFTER metadata was updated but BEFORE lock was released
    // The state is actually consistent, so we just need to release the lock
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_completed_write_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    let namespace = format!("{base_namespace}:0");
    let lock_key = format!("{namespace}:write_lock");
    let block_key = format!("{namespace}:block");

    // Simulate completed write: block number is already set to target
    redis::cmd("SET")
        .arg(&block_key)
        .arg("100")
        .query::<()>(&mut conn)
        .unwrap();

    // Insert a stale lock for the same block (crash after metadata, before lock release)
    let old_timestamp = chrono::Utc::now().timestamp() - 120;
    let lock_json = serde_json::json!({
        "target_block": 100,
        "started_at": old_timestamp,
        "writer_id": "crashed-writer"
    })
    .to_string();

    redis::cmd("SET")
        .arg(&lock_key)
        .arg(&lock_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Recover - should just release the lock since state is consistent
    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 100);
    assert_eq!(recoveries[0].previous_block, Some(100)); // Same as target

    // Verify lock was cleared
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key)
        .query(&mut conn)
        .unwrap();
    assert!(!exists, "lock should be cleared after recovery");
}

#[tokio::test]
async fn test_recover_stale_locks_repairs_state_with_diffs() {
    // Test case: crash happened during write, need to re-apply diffs
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_repair_with_diffs_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    let namespace = format!("{base_namespace}:0");
    let lock_key = format!("{namespace}:write_lock");

    // Insert a stale lock for block 0 (no previous block)
    let old_timestamp = chrono::Utc::now().timestamp() - 120;
    let lock_json = serde_json::json!({
        "target_block": 0,
        "started_at": old_timestamp,
        "writer_id": "crashed-writer"
    })
    .to_string();

    redis::cmd("SET")
        .arg(&lock_key)
        .arg(&lock_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Store the diff that will be needed for recovery
    let diff_key = format!("{base_namespace}:diff:0");
    let diff = BlockStateUpdate::new(0, B256::from([1u8; 32]), B256::from([2u8; 32]));
    let diff_json = serde_json::to_string(&diff).unwrap();

    redis::cmd("SET")
        .arg(&diff_key)
        .arg(&diff_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Recover
    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 0);
    assert_eq!(recoveries[0].previous_block, None);

    // Verify lock was cleared
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key)
        .query(&mut conn)
        .unwrap();
    assert!(!exists, "lock should be cleared after recovery");

    // Verify block metadata was written
    let block_key = format!("{namespace}:block");
    let block_num: Option<String> = redis::cmd("GET").arg(&block_key).query(&mut conn).unwrap();
    assert_eq!(block_num, Some("0".to_string()));
}

#[tokio::test]
async fn test_recover_stale_locks_fails_without_diff() {
    // Test case: crash happened during write, but diff is missing - cannot repair
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_missing_diff_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    let namespace = format!("{base_namespace}:0");
    let lock_key = format!("{namespace}:write_lock");

    // Insert a stale lock for block 5 (no diff stored)
    let old_timestamp = chrono::Utc::now().timestamp() - 120;
    let lock_json = serde_json::json!({
        "target_block": 5,
        "started_at": old_timestamp,
        "writer_id": "crashed-writer"
    })
    .to_string();

    redis::cmd("SET")
        .arg(&lock_key)
        .arg(&lock_json)
        .query::<()>(&mut conn)
        .unwrap();

    // DO NOT store the diff

    // Recover
    let result = writer.recover_stale_locks();

    assert!(result.is_err(), "recovery should fail without diff");
    let err = result.unwrap_err();
    assert!(
        matches!(
            err,
            state_store::common::error::StateError::MissingStateDiff { .. }
        ),
        "expected MissingStateDiff error, got: {err:?}",
    );

    // Verify lock is STILL in place (protecting readers from corrupt state)
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key)
        .query(&mut conn)
        .unwrap();
    assert!(exists, "lock should remain when recovery fails");
}

#[tokio::test]
async fn test_force_recover_namespace_specific_index() {
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_force_recover_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    // Insert stale locks in namespaces 0 and 2
    let old_timestamp = chrono::Utc::now().timestamp() - 120;

    for idx in [0, 2] {
        let namespace = format!("{base_namespace}:{idx}");
        let lock_key = format!("{namespace}:write_lock");
        let block_key = format!("{namespace}:block");

        // Set block to simulate completed write (so recovery doesn't need diffs)
        let target_block = 100 + idx as u64;
        redis::cmd("SET")
            .arg(&block_key)
            .arg(target_block.to_string())
            .query::<()>(&mut conn)
            .unwrap();

        let lock_json = serde_json::json!({
            "target_block": target_block,
            "started_at": old_timestamp,
            "writer_id": format!("writer-{}", idx)
        })
        .to_string();

        redis::cmd("SET")
            .arg(&lock_key)
            .arg(&lock_json)
            .query::<()>(&mut conn)
            .unwrap();
    }

    // Force recover only namespace 2
    let recovery = writer.force_recover_namespace(2).unwrap();

    assert!(recovery.is_some());
    let recovery = recovery.unwrap();
    assert_eq!(recovery.namespace, format!("{base_namespace}:2"));
    assert_eq!(recovery.target_block, 102);

    // Verify namespace 0 still has its lock
    let lock_key_0 = format!("{base_namespace}:0:write_lock");
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key_0)
        .query(&mut conn)
        .unwrap();
    assert!(exists, "namespace 0 lock should still exist");

    // Verify namespace 2 lock was cleared
    let lock_key_2 = format!("{base_namespace}:2:write_lock");
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key_2)
        .query(&mut conn)
        .unwrap();
    assert!(!exists, "namespace 2 lock should be cleared");
}

#[tokio::test]
async fn test_recovery_applies_multiple_diffs() {
    // Test case: namespace at block 97, crashed during write of block 100
    // Recovery needs to apply diffs for blocks 98, 99, and 100
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_multi_diff_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    // Namespace 1 (100 % 3 = 1)
    let namespace = format!("{base_namespace}:1");
    let lock_key = format!("{namespace}:write_lock");
    let block_key = format!("{namespace}:block");

    // Set namespace to block 97
    redis::cmd("SET")
        .arg(&block_key)
        .arg("97")
        .query::<()>(&mut conn)
        .unwrap();

    // Insert stale lock for block 100
    let old_timestamp = chrono::Utc::now().timestamp() - 120;
    let lock_json = serde_json::json!({
        "target_block": 100,
        "started_at": old_timestamp,
        "writer_id": "crashed-writer"
    })
    .to_string();

    redis::cmd("SET")
        .arg(&lock_key)
        .arg(&lock_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Store diffs for blocks 98, 99, 100
    for block_num in 98..=100 {
        let diff_key = format!("{base_namespace}:diff:{block_num}");
        let diff = BlockStateUpdate::new(
            block_num,
            B256::from([block_num as u8; 32]),
            B256::from([(block_num + 100) as u8; 32]),
        );
        let diff_json = serde_json::to_string(&diff).unwrap();

        redis::cmd("SET")
            .arg(&diff_key)
            .arg(&diff_json)
            .query::<()>(&mut conn)
            .unwrap();
    }

    // Recover
    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 100);
    assert_eq!(recoveries[0].previous_block, Some(97));

    // Verify namespace is now at block 100
    let block_num: Option<String> = redis::cmd("GET").arg(&block_key).query(&mut conn).unwrap();
    assert_eq!(block_num, Some("100".to_string()));

    // Verify lock was cleared
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key)
        .query(&mut conn)
        .unwrap();
    assert!(!exists, "lock should be cleared after recovery");
}

#[tokio::test]
async fn test_recovery_partial_diff_chain_fails() {
    // Test case: namespace at block 97, crashed during write of block 100
    // But only diff for block 98 exists, 99 is missing - should fail
    let instance = LocalInstance::new()
        .await
        .expect("Failed to start instance");
    let base_namespace = format!("test_partial_diff_{}", uuid::Uuid::new_v4());
    let config = CircularBufferConfig::new(3).unwrap();

    let chunked_config = ChunkedWriteConfig::new(1000, 60);

    let writer = StateWriter::with_chunked_config(
        &instance.redis_url,
        &base_namespace,
        config.clone(),
        chunked_config,
    )
    .unwrap();

    let client = redis::Client::open(instance.redis_url.as_str()).unwrap();
    let mut conn = client.get_connection().unwrap();

    // Namespace 1 (100 % 3 = 1)
    let namespace = format!("{base_namespace}:1");
    let lock_key = format!("{namespace}:write_lock");
    let block_key = format!("{namespace}:block");

    // Set namespace to block 97
    redis::cmd("SET")
        .arg(&block_key)
        .arg("97")
        .query::<()>(&mut conn)
        .unwrap();

    // Insert stale lock for block 100
    let old_timestamp = chrono::Utc::now().timestamp() - 120;
    let lock_json = serde_json::json!({
        "target_block": 100,
        "started_at": old_timestamp,
        "writer_id": "crashed-writer"
    })
    .to_string();

    redis::cmd("SET")
        .arg(&lock_key)
        .arg(&lock_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Only store diff for block 98 (missing 99 and 100)
    let diff_key = format!("{base_namespace}:diff:98");
    let diff = BlockStateUpdate::new(98, B256::from([98u8; 32]), B256::from([198u8; 32]));
    let diff_json = serde_json::to_string(&diff).unwrap();

    redis::cmd("SET")
        .arg(&diff_key)
        .arg(&diff_json)
        .query::<()>(&mut conn)
        .unwrap();

    // Recover - should fail because diff 99 is missing
    let result = writer.recover_stale_locks();

    assert!(result.is_err(), "recovery should fail with partial diffs");
    let err = result.unwrap_err();
    match err {
        state_store::common::error::StateError::MissingStateDiff {
            needed_block,
            target_block,
        } => {
            assert_eq!(needed_block, 99);
            assert_eq!(target_block, 100);
        }
        _ => panic!("expected MissingStateDiff error, got: {err:?}"),
    }

    // Verify lock is STILL in place
    let exists: bool = redis::cmd("EXISTS")
        .arg(&lock_key)
        .query(&mut conn)
        .unwrap();
    assert!(exists, "lock should remain when recovery fails");
}

// =============================================================================
// TEST UTILITIES
// =============================================================================

struct TestContext {
    redis_url: String,
    base_namespace: String,
    buffer_size: usize,
    _container: ContainerAsync<Redis>,
}

impl TestContext {
    async fn new(test_name: &str, buffer_size: usize) -> Self {
        let container = Redis::default()
            .start()
            .await
            .expect("Failed to start Redis");

        let host = container.get_host().await.unwrap();
        let port = container.get_host_port_ipv4(6379).await.unwrap();
        let redis_url = format!("redis://{host}:{port}");
        let base_namespace = format!("test_{test_name}_{}", uuid::Uuid::new_v4());

        Self {
            redis_url,
            base_namespace,
            buffer_size,
            _container: container,
        }
    }

    fn conn(&self) -> redis::Connection {
        let client = redis::Client::open(self.redis_url.as_str()).unwrap();
        client.get_connection().unwrap()
    }

    fn writer(&self) -> StateWriter {
        StateWriter::with_chunked_config(
            &self.redis_url,
            &self.base_namespace,
            CircularBufferConfig::new(self.buffer_size).unwrap(),
            ChunkedWriteConfig::new(1000, 60), // 60s stale timeout
        )
        .unwrap()
    }

    fn reader(&self) -> StateReader {
        StateReader::new(
            &self.redis_url,
            &self.base_namespace,
            CircularBufferConfig::new(self.buffer_size).unwrap(),
        )
        .unwrap()
    }

    fn namespace(&self, idx: usize) -> String {
        format!("{}:{}", self.base_namespace, idx)
    }

    /// Set block number for a namespace
    fn set_namespace_block(&self, conn: &mut redis::Connection, idx: usize, block: u64) {
        let ns = self.namespace(idx);
        let block_key = format!("{ns}:block");
        redis::cmd("SET")
            .arg(&block_key)
            .arg(block.to_string())
            .query::<()>(conn)
            .unwrap();
    }

    /// Get block number for a namespace
    fn get_namespace_block(&self, conn: &mut redis::Connection, idx: usize) -> Option<u64> {
        let ns = self.namespace(idx);
        let block_key = format!("{ns}:block");
        redis::cmd("GET")
            .arg(&block_key)
            .query::<Option<String>>(conn)
            .ok()
            .flatten()
            .and_then(|s| s.parse().ok())
    }

    /// Insert a stale lock
    fn insert_stale_lock(
        &self,
        conn: &mut redis::Connection,
        idx: usize,
        target_block: u64,
        age_secs: i64,
    ) {
        let ns = self.namespace(idx);
        let lock_key = format!("{ns}:write_lock");
        let timestamp = chrono::Utc::now().timestamp() - age_secs;
        let lock_json = json!({
            "target_block": target_block,
            "started_at": timestamp,
            "writer_id": format!("crashed-writer-{}", idx)
        })
        .to_string();
        redis::cmd("SET")
            .arg(&lock_key)
            .arg(&lock_json)
            .query::<()>(conn)
            .unwrap();
    }

    /// Check if lock exists
    fn has_lock(&self, conn: &mut redis::Connection, idx: usize) -> bool {
        let ns = self.namespace(idx);
        let lock_key = format!("{ns}:write_lock");
        redis::cmd("EXISTS")
            .arg(&lock_key)
            .query::<bool>(conn)
            .unwrap()
    }

    /// Store a state diff
    fn store_diff(&self, conn: &mut redis::Connection, update: &BlockStateUpdate) {
        let diff_key = format!("{}:diff:{}", self.base_namespace, update.block_number);
        let diff_json = serde_json::to_string(update).unwrap();
        redis::cmd("SET")
            .arg(&diff_key)
            .arg(&diff_json)
            .query::<()>(conn)
            .unwrap();
    }

    /// Check if diff exists
    fn has_diff(&self, conn: &mut redis::Connection, block: u64) -> bool {
        let diff_key = format!("{}:diff:{}", self.base_namespace, block);
        redis::cmd("EXISTS")
            .arg(&diff_key)
            .query::<bool>(conn)
            .unwrap()
    }

    /// Write a complete valid namespace state (accounts + metadata)
    fn write_valid_namespace_state(
        &self,
        conn: &mut redis::Connection,
        idx: usize,
        block: u64,
        accounts: &[AccountState],
    ) {
        let ns = self.namespace(idx);

        // Write accounts
        for account in accounts {
            let account_key = get_account_key(&ns, &account.address_hash);
            redis::cmd("HSET")
                .arg(&account_key)
                .arg("balance")
                .arg(account.balance.to_string())
                .arg("nonce")
                .arg(account.nonce.to_string())
                .arg("code_hash")
                .arg(format!("0x{}", hex::encode(account.code_hash)))
                .query::<()>(conn)
                .unwrap();
        }

        // Write block number
        self.set_namespace_block(conn, idx, block);

        // Write block hash and state root
        let hash_key = format!("{}:block_hash:{}", self.base_namespace, block);
        let root_key = format!("{}:state_root:{}", self.base_namespace, block);
        redis::cmd("SET")
            .arg(&hash_key)
            .arg(format!("0x{block:064x}"))
            .query::<()>(conn)
            .unwrap();
        redis::cmd("SET")
            .arg(&root_key)
            .arg(format!("0x{:064x}", block + 1000))
            .query::<()>(conn)
            .unwrap();
    }
}

fn make_test_account(seed: u8) -> AccountState {
    AccountState {
        address_hash: B256::from([seed; 32]).into(),
        balance: U256::from(1000u64 * seed as u64),
        nonce: seed as u64,
        code_hash: B256::ZERO,
        code: None,
        storage: HashMap::new(),
        deleted: false,
    }
}

fn make_block_update(block: u64) -> BlockStateUpdate {
    BlockStateUpdate {
        block_number: block,
        block_hash: B256::from([block as u8; 32]),
        state_root: B256::from([(block + 100) as u8; 32]),
        accounts: vec![make_test_account(block as u8)],
    }
}

// =============================================================================
// TEST: CLEAN STATE - NO RECOVERY NEEDED
// =============================================================================

#[tokio::test]
async fn test_recovery_clean_state_no_action_needed() {
    let ctx = TestContext::new("clean_state", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Setup: All namespaces have correct blocks for highest_valid = 100
    // ns0 = 99, ns1 = 100, ns2 = 98
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 2, 98, &accounts);

    // Store diffs for recent blocks (in case recovery checks them)
    for block in 98..=100 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // Recovery should succeed with no changes
    let recoveries = writer.recover_stale_locks().unwrap();
    assert!(recoveries.is_empty(), "No stale locks should be found");

    // Verify all blocks are still correct
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(99));
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(98));
}

// =============================================================================
// TEST: STALE LOCK - WRITE ALREADY COMPLETED
// =============================================================================

#[tokio::test]
async fn test_recovery_stale_lock_write_completed() {
    let ctx = TestContext::new("lock_completed", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Scenario: Crash happened AFTER metadata update but BEFORE lock release
    // Block is already at target, just need to release lock

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 100, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 100, 120); // 2 min old lock

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 100);
    assert_eq!(recoveries[0].previous_block, Some(100)); // Same as target
    assert!(!ctx.has_lock(&mut conn, 0), "Lock should be cleared");
}

// =============================================================================
// TEST: STALE LOCK - NEEDS SINGLE DIFF TO COMPLETE
// =============================================================================

#[tokio::test]
async fn test_recovery_stale_lock_needs_single_diff() {
    let ctx = TestContext::new("single_diff", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Namespace 2 already at block 2, crashed during write of block 5
    // (5 % 3 = 2, so block 5 goes to namespace 2)
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 2, 2, &accounts);
    ctx.insert_stale_lock(&mut conn, 2, 5, 120);

    // Need diffs from (2+1) to 5 = diffs 3, 4, 5
    for block in 3..=5 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 5);
    assert_eq!(recoveries[0].previous_block, Some(2));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(5));
}

// =============================================================================
// TEST: STALE LOCK - NEEDS MULTIPLE DIFFS
// =============================================================================

#[tokio::test]
async fn test_recovery_stale_lock_needs_multiple_diffs() {
    let ctx = TestContext::new("multi_diff", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Scenario: Namespace at block 97, crashed during write of block 100
    // Need to apply diffs 98, 99, 100

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Store all required diffs
    for block in 98..=100 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 100);
    assert_eq!(recoveries[0].previous_block, Some(97));
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
    assert!(!ctx.has_lock(&mut conn, 1));
}

// =============================================================================
// TEST: STALE LOCK - MISSING DIFF FAILS (LOCK PRESERVED)
// =============================================================================

#[tokio::test]
async fn test_recovery_missing_diff_preserves_lock() {
    let ctx = TestContext::new("missing_diff", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Scenario: Need diff for block 50, but it doesn't exist
    // CRITICAL: Lock must remain to protect readers from corrupt state

    ctx.insert_stale_lock(&mut conn, 0, 50, 120);
    // DO NOT store the diff

    let result = writer.recover_stale_locks();

    assert!(result.is_err());
    match result.unwrap_err() {
        state_store::common::error::StateError::MissingStateDiff {
            needed_block,
            target_block,
        } => {
            assert_eq!(needed_block, 0); // First block needed
            assert_eq!(target_block, 50);
        }
        e => panic!("Expected MissingStateDiff, got: {e:?}"),
    }

    // CRITICAL: Lock must still exist
    assert!(ctx.has_lock(&mut conn, 0), "Lock MUST remain on failure");
}

// =============================================================================
// TEST: MULTIPLE STALE LOCKS - ALL RECOVERED
// =============================================================================

#[tokio::test]
async fn test_recovery_multiple_stale_locks() {
    let ctx = TestContext::new("multi_locks", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Scenario: Two namespaces have stale locks

    let accounts = vec![make_test_account(1)];

    // Namespace 0: completed write
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 99, 120);

    // Namespace 2: needs diff application
    ctx.write_valid_namespace_state(&mut conn, 2, 95, &accounts);
    ctx.insert_stale_lock(&mut conn, 2, 98, 120);
    for block in 96..=98 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 2);
    assert!(!ctx.has_lock(&mut conn, 0));
    assert!(!ctx.has_lock(&mut conn, 2));
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(99));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(98));
}

// =============================================================================
// TEST: FORCE RECOVER SPECIFIC NAMESPACE
// =============================================================================

#[tokio::test]
async fn test_force_recover_specific_namespace() {
    let ctx = TestContext::new("force_recover", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Stale locks on ns 0 and ns 2
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 2, 98, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 99, 120);
    ctx.insert_stale_lock(&mut conn, 2, 98, 120);

    // Only recover namespace 2
    let recovery = writer.force_recover_namespace(2).unwrap();

    assert!(recovery.is_some());
    assert_eq!(recovery.unwrap().target_block, 98);
    assert!(!ctx.has_lock(&mut conn, 2));
    assert!(ctx.has_lock(&mut conn, 0), "Namespace 0 lock should remain");
}

// =============================================================================
// TEST: NAMESPACE REPAIR - FIRST NAMESPACE CORRUPTED
// =============================================================================

#[tokio::test]
async fn test_repair_first_namespace_corrupted() {
    let ctx = TestContext::new("first_corrupted", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Buffer: ns0, ns1, ns2 with buffer_size=3
    // highest_valid_block = 100 (in ns1)
    // Expected: ns0=99, ns1=100, ns2=98
    // Corrupt: ns0 has wrong block (96 instead of 99)

    let accounts = vec![make_test_account(1)];

    // ns0 is WRONG (should be 99, but has 96)
    ctx.write_valid_namespace_state(&mut conn, 0, 96, &accounts);
    // ns1 is VALID (highest)
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);
    // ns2 is correct
    ctx.write_valid_namespace_state(&mut conn, 2, 98, &accounts);

    // Add block 99 to trace provider (for fetching full state)
    provider.add_block(make_block_update(99));

    // Find valid namespace
    let (valid_idx, valid_block) = writer
        .find_valid_namespace_state()
        .unwrap()
        .expect("Should find valid namespace");

    assert_eq!(valid_idx, 1);
    assert_eq!(valid_block, 100);

    // ns0 expected block is 99
    let expected_ns0 = writer.expected_block_for_namespace(0, valid_block);
    assert_eq!(expected_ns0, 99);

    // Since valid_block (100) > expected_block (99), we CANNOT use
    // repair_namespace_from_valid_state(). We must fetch full state.
    assert!(
        valid_block > expected_ns0,
        "This test verifies the source > target case"
    );

    // Fetch full state for block 99 from trace provider
    let update = provider
        .fetch_block_state(expected_ns0)
        .await
        .expect("Provider should have block 99");

    // Write full state directly to namespace 0
    writer
        .write_full_state_to_namespace(0, &update)
        .expect("Should write full state to ns0");

    // Verify ns0 is now at block 99
    assert_eq!(
        ctx.get_namespace_block(&mut conn, 0),
        Some(99),
        "Namespace 0 should be at block 99 after repair"
    );
}

// =============================================================================
// TEST: NAMESPACE REPAIR - LAST NAMESPACE CORRUPTED
// =============================================================================

#[tokio::test]
async fn test_repair_last_namespace_corrupted() {
    let ctx = TestContext::new("last_corrupted", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    let accounts = vec![make_test_account(1)];

    // Setup:
    // - ns0 at 99 (correct for valid_block=100)
    // - ns1 at 100 (VALID - highest block)
    // - ns2 at 50 (CORRUPTED - should be 98)
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 2, 50, &accounts); // WRONG!

    // Add block 98 to the trace provider - this is what we'll fetch
    provider.add_block(make_block_update(98));

    // Find the valid namespace
    let (valid_idx, valid_block) = writer
        .find_valid_namespace_state()
        .unwrap()
        .expect("Should find valid namespace");

    // ns1 has the highest block, so it's the valid namespace
    assert_eq!(valid_idx, 1);
    assert_eq!(valid_block, 100);

    // Calculate expected blocks:
    // With buffer_size=3, valid_block=100, highest_ns_idx = 100 % 3 = 1
    // ns0: 100 - (1 - 0) = 99 
    // ns1: 100 (valid) 
    // ns2: 100 - (3 - (2 - 1)) = 100 - 2 = 98
    let expected_ns2 = writer.expected_block_for_namespace(2, valid_block);
    assert_eq!(expected_ns2, 98, "ns2 should expect block 98");

    // Simulate the FIXED repair_corrupted_namespaces() logic
    for namespace_idx in 0..writer.buffer_size() {
        if namespace_idx == valid_idx {
            continue;
        }

        let expected_block = writer.expected_block_for_namespace(namespace_idx, valid_block);
        let actual_block = writer.get_namespace_block(namespace_idx).unwrap();

        if actual_block != Some(expected_block) {
            // KEY FIX: Check if source is before or after target
            if valid_block < expected_block {
                // Case 1: Source BEFORE target - copy and apply diffs forward
                for block_num in (valid_block + 1)..=expected_block {
                    if !writer.has_diff(block_num).unwrap() {
                        let update = provider.fetch_block_state(block_num).await.unwrap();
                        writer.store_diff_only(&update).unwrap();
                    }
                }
                writer
                    .repair_namespace_from_valid_state(namespace_idx, valid_idx, expected_block)
                    .unwrap();
            } else {
                // Case 2: Source AFTER target - fetch full state directly
                // This is the critical fix! We can't copy forward from block 100 to get block 98.
                // Instead, fetch block 98's full state from the trace provider.
                let update = provider
                    .fetch_block_state(expected_block)
                    .await
                    .expect(&format!("Provider should have block {expected_block}"));

                // Store diff for future recovery needs
                writer.store_diff_only(&update).unwrap();

                // Write full state directly to the namespace
                writer
                    .write_full_state_to_namespace(namespace_idx, &update)
                    .expect("Should write full state");
            }
        }
    }

    // Verify ns2 is now at the correct block
    assert_eq!(
        ctx.get_namespace_block(&mut conn, 2),
        Some(98),
        "ns2 should be repaired to block 98"
    );

    // Verify ns0 is still correct (it was already at 99)
    assert_eq!(
        ctx.get_namespace_block(&mut conn, 0),
        Some(99),
        "ns0 should still be at block 99"
    );

    // Verify trace provider was called for block 98
    let history = provider.get_fetch_history();
    assert!(
        history.contains(&98),
        "Should have fetched block 98 from provider"
    );
}

// =============================================================================
// TEST: NAMESPACE REPAIR - MULTIPLE CORRUPTED
// =============================================================================

#[tokio::test]
async fn test_repair_multiple_namespaces_corrupted() {
    let ctx = TestContext::new("multi_corrupted", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Only ns1 at block 100 is valid
    // ns0 and ns2 are both corrupted

    let accounts = vec![make_test_account(1)];

    ctx.write_valid_namespace_state(&mut conn, 0, 50, &accounts); // WRONG
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts); // VALID
    ctx.write_valid_namespace_state(&mut conn, 2, 60, &accounts); // WRONG

    // Store all needed diffs
    for block in 98..=100 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // For this to work, we need source blocks that are BEFORE targets
    // ns0 should be 99 (need source < 99)
    // ns2 should be 98 (need source < 98)
    //
    // The issue is ns1=100 can't be used as source for ns0=99 or ns2=98
    // because 100 > 99 and 100 > 98.
    //
    // In real scenario, we'd need an older snapshot.
    // For testing, let's verify the expected_block calculations and
    // test the repair with a valid older source.

    // Let's change ns1 to block 97 as the valid source
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);

    // Now expected blocks are different:
    // highest_valid = 97 (ns1)
    // ns0: 97 - (97%3 - 0) = 97 - (1 - 0) = 96
    // ns1: 97 (valid)
    // ns2: 97 - (3 - (2 - 1)) = 97 - 2 = 95

    // Repair ns0 from ns1 to reach 96
    ctx.store_diff(&mut conn, &make_block_update(96));
    ctx.store_diff(&mut conn, &make_block_update(97));

    writer.repair_namespace_from_valid_state(0, 1, 98).unwrap();
    writer.repair_namespace_from_valid_state(2, 1, 99).unwrap();

    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(98));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(99));
}

// =============================================================================
// TEST: REPAIR FAILS WITHOUT REQUIRED DIFFS
// =============================================================================

#[tokio::test]
async fn test_repair_fails_without_diffs() {
    let ctx = TestContext::new("repair_no_diffs", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 95, &accounts);

    // Try to repair ns0 to block 99 from ns1 at 95
    // Need diffs 96, 97, 98, 99 - but none exist

    let result = writer.repair_namespace_from_valid_state(0, 1, 99);

    assert!(result.is_err());
    match result.unwrap_err() {
        state_store::common::error::StateError::MissingStateDiff { needed_block, .. } => {
            assert_eq!(needed_block, 96); // First missing diff
        }
        e => panic!("Expected MissingStateDiff, got: {e:?}"),
    }
}

// =============================================================================
// TEST: REPAIR VERIFIES DIFFS BEFORE MODIFYING STATE
// =============================================================================

#[tokio::test]
async fn test_repair_verifies_diffs_atomically() {
    let ctx = TestContext::new("atomic_verify", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 90, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 1, 95, &accounts);

    // Store diffs 96, 97 but NOT 98 or 99
    ctx.store_diff(&mut conn, &make_block_update(96));
    ctx.store_diff(&mut conn, &make_block_update(97));

    // Try to repair ns0 to block 99
    let result = writer.repair_namespace_from_valid_state(0, 1, 99);
    assert!(result.is_err());

    // CRITICAL: ns0 should NOT have been modified
    // The repair should verify ALL diffs exist BEFORE starting to copy
    assert_eq!(
        ctx.get_namespace_block(&mut conn, 0),
        Some(90),
        "Namespace should not be modified when repair fails"
    );
}

// =============================================================================
// TEST: CIRCULAR BUFFER WRAP RECOVERY
// =============================================================================

#[tokio::test]
async fn test_recovery_after_buffer_wrap() {
    let ctx = TestContext::new("buffer_wrap", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // After many blocks, buffer has wrapped multiple times
    // Current state: ns0=300, ns1=301, ns2=299
    // Crash during write of block 303 to ns0

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 300, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 1, 301, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 2, 299, &accounts);

    ctx.insert_stale_lock(&mut conn, 0, 303, 120);

    // Store required diffs
    for block in 301..=303 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(303));
}

// =============================================================================
// TEST: EARLY CHAIN (BLOCKS < BUFFER_SIZE)
// =============================================================================

#[tokio::test]
async fn test_recovery_early_chain() {
    let ctx = TestContext::new("early_chain", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Very early in chain, namespace 1 at block 1, crashed during write of block 4
    // (4 % 3 = 1, so block 4 goes to namespace 1)

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 1, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 4, 120);

    // Need diffs from 2 to 4
    for block in 2..=4 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 4);
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(4));
}

// =============================================================================
// TEST: FIND VALID NAMESPACE
// =============================================================================

#[tokio::test]
async fn test_find_valid_namespace_skips_locked() {
    let ctx = TestContext::new("find_valid", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];

    // ns0 has lock (should be skipped)
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 102, 10); // Not stale yet

    // ns1 is valid
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);

    // ns2 has lock
    ctx.write_valid_namespace_state(&mut conn, 2, 98, &accounts);
    ctx.insert_stale_lock(&mut conn, 2, 101, 10);

    let result = writer.find_valid_namespace_state().unwrap();

    assert!(result.is_some());
    let (idx, block) = result.unwrap();
    assert_eq!(idx, 1, "Should find ns1 as the only unlocked namespace");
    assert_eq!(block, 100);
}

#[tokio::test]
async fn test_find_valid_namespace_returns_highest_block() {
    let ctx = TestContext::new("find_highest", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];

    ctx.write_valid_namespace_state(&mut conn, 0, 50, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);
    ctx.write_valid_namespace_state(&mut conn, 2, 75, &accounts);

    let result = writer.find_valid_namespace_state().unwrap();

    assert!(result.is_some());
    let (idx, block) = result.unwrap();
    assert_eq!(idx, 1, "Should find ns1 with highest block");
    assert_eq!(block, 100);
}

#[tokio::test]
async fn test_find_valid_namespace_none_when_all_locked() {
    let ctx = TestContext::new("all_locked", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];

    for idx in 0..3 {
        ctx.write_valid_namespace_state(&mut conn, idx, 100 + idx as u64, &accounts);
        ctx.insert_stale_lock(&mut conn, idx, 105, 10); // Active locks
    }

    let result = writer.find_valid_namespace_state().unwrap();
    assert!(
        result.is_none(),
        "Should find no valid namespace when all are locked"
    );
}

// =============================================================================
// TEST: EXPECTED BLOCK CALCULATION
// =============================================================================

#[tokio::test]
async fn test_expected_block_calculation() {
    let ctx = TestContext::new("expected_calc", 3).await;
    let writer = ctx.writer();

    // With highest_valid_block = 100 (in ns1)
    // ns0 should be 99, ns1 should be 100, ns2 should be 98
    assert_eq!(writer.expected_block_for_namespace(0, 100), 99);
    assert_eq!(writer.expected_block_for_namespace(1, 100), 100);
    assert_eq!(writer.expected_block_for_namespace(2, 100), 98);

    // With highest_valid_block = 99 (in ns0)
    assert_eq!(writer.expected_block_for_namespace(0, 99), 99);
    assert_eq!(writer.expected_block_for_namespace(1, 99), 97);
    assert_eq!(writer.expected_block_for_namespace(2, 99), 98);

    // Early chain: highest = 2
    assert_eq!(writer.expected_block_for_namespace(0, 2), 0);
    assert_eq!(writer.expected_block_for_namespace(1, 2), 1);
    assert_eq!(writer.expected_block_for_namespace(2, 2), 2);
}

// =============================================================================
// TEST: STORE DIFF ONLY (FOR RECOVERY)
// =============================================================================

#[tokio::test]
async fn test_store_diff_only() {
    let ctx = TestContext::new("store_diff", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let update = make_block_update(42);

    writer.store_diff_only(&update).unwrap();

    assert!(ctx.has_diff(&mut conn, 42));
    assert!(writer.has_diff(42).unwrap());
}

// =============================================================================
// TEST: LARGE GAP RECOVERY
// =============================================================================

#[tokio::test]
async fn test_recovery_large_gap() {
    let ctx = TestContext::new("large_gap", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Namespace at block 90, need to reach 100
    // This is a large gap (10 blocks)

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 90, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Store all 10 diffs
    for block in 91..=100 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
}

// =============================================================================
// TEST: STATE INTEGRITY AFTER RECOVERY
// =============================================================================

#[tokio::test]
async fn test_recovery_preserves_account_state() {
    let ctx = TestContext::new("state_integrity", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Setup: namespace 0 at block 99, crashed during write of block 102
    // (102 % 3 = 0)
    let account = make_test_account(42);
    let accounts = vec![account.clone()];

    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 102, 120);

    // Need diffs from 100 to 102
    for block in 100..=101 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // Block 102 has the updated account
    let update = BlockStateUpdate {
        block_number: 102,
        block_hash: B256::from([102u8; 32]),
        state_root: B256::from([202u8; 32]),
        accounts: vec![AccountState {
            address_hash: account.address_hash.clone(),
            balance: U256::from(9999u64),
            nonce: 100,
            code_hash: B256::ZERO,
            code: None,
            storage: HashMap::new(),
            deleted: false,
        }],
    };
    ctx.store_diff(&mut conn, &update);

    writer.recover_stale_locks().unwrap();

    // Verify the account state was updated
    let reader = ctx.reader();
    let recovered = reader
        .get_account(account.address_hash, 102)
        .unwrap()
        .expect("Account should exist");

    assert_eq!(recovered.balance, U256::from(9999u64));
    assert_eq!(recovered.nonce, 100);
}

// =============================================================================
// TEST: CONCURRENT WRITE PROTECTION
// =============================================================================

#[tokio::test]
async fn test_active_lock_blocks_recovery() {
    let ctx = TestContext::new("active_lock", 3).await;
    let mut conn = ctx.conn();

    // Create writer with 60s stale timeout
    let writer = ctx.writer();

    // Insert a lock that's only 10 seconds old (not stale)
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &[make_test_account(1)]);
    ctx.insert_stale_lock(&mut conn, 0, 100, 10);

    // Recovery should not touch this lock
    let recoveries = writer.recover_stale_locks().unwrap();

    assert!(recoveries.is_empty(), "Active lock should not be recovered");
    assert!(ctx.has_lock(&mut conn, 0), "Active lock should remain");
}

// =============================================================================
// TEST: RECOVERY FROM GENESIS
// =============================================================================

#[tokio::test]
async fn test_recovery_from_genesis_block() {
    let ctx = TestContext::new("genesis", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Crash during write of block 0 (genesis)
    ctx.insert_stale_lock(&mut conn, 0, 0, 120);

    let genesis_update = BlockStateUpdate {
        block_number: 0,
        block_hash: B256::ZERO,
        state_root: B256::from([1u8; 32]),
        accounts: vec![make_test_account(1)],
    };
    ctx.store_diff(&mut conn, &genesis_update);

    let recoveries = writer.recover_stale_locks().unwrap();

    assert_eq!(recoveries.len(), 1);
    assert_eq!(recoveries[0].target_block, 0);
    assert_eq!(recoveries[0].previous_block, None);
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(0));
}

// =============================================================================
// TEST: REPAIR SOURCE VALIDATION
// =============================================================================

#[tokio::test]
async fn test_repair_source_must_be_before_target() {
    let ctx = TestContext::new("source_before_target", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];

    // Source at block 100, trying to repair to 95 - INVALID!
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);

    let result = writer.repair_namespace_from_valid_state(0, 1, 95);

    assert!(result.is_err(), "Source block must be before target block");
}

// =============================================================================
// TEST: INVALID NAMESPACE INDEX
// =============================================================================

#[tokio::test]
async fn test_invalid_namespace_index() {
    let ctx = TestContext::new("invalid_ns", 3).await;
    let writer = ctx.writer();

    // Buffer size is 3, so index 5 is invalid
    let result = writer.force_recover_namespace(5);

    assert!(result.is_err());
    match result.unwrap_err() {
        state_store::common::error::StateError::InvalidNamespace(idx, size) => {
            assert_eq!(idx, 5);
            assert_eq!(size, 3);
        }
        e => panic!("Expected InvalidNamespace error, got: {e:?}"),
    }
}

#[tokio::test]
async fn test_repair_invalid_namespace_index() {
    let ctx = TestContext::new("repair_invalid", 3).await;
    let writer = ctx.writer();

    let result = writer.repair_namespace_from_valid_state(10, 0, 100);

    assert!(result.is_err());
}

// =============================================================================
// TEST: EMPTY NAMESPACE (NO ACCOUNTS) DETECTION
// =============================================================================

#[tokio::test]
async fn test_empty_namespace_not_considered_valid() {
    let ctx = TestContext::new("empty_ns", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Set block number but NO accounts
    ctx.set_namespace_block(&mut conn, 0, 100);

    // Namespace with accounts
    ctx.write_valid_namespace_state(&mut conn, 1, 99, &[make_test_account(1)]);

    let result = writer.find_valid_namespace_state().unwrap();

    assert!(result.is_some());
    let (idx, block) = result.unwrap();
    assert_eq!(idx, 1, "Should skip ns0 with no accounts");
    assert_eq!(block, 99);
}

// =============================================================================
// TEST: DIFF CLEANUP AFTER BUFFER ROTATION
// =============================================================================

#[tokio::test]
async fn test_old_diffs_cleaned_up() {
    let ctx = TestContext::new("diff_cleanup", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // With buffer_size=3, diffs older than (current - 3) should be deleted
    // When storing diff for block 100, diff for block 97 should be deleted

    // First, manually store an old diff
    ctx.store_diff(&mut conn, &make_block_update(97));
    assert!(ctx.has_diff(&mut conn, 97));

    // Commit block 100 through normal flow would trigger cleanup
    // For this test, we use store_diff_only which doesn't clean up
    // The cleanup happens in commit_block

    // Verify the old diff still exists after store_diff_only
    writer.store_diff_only(&make_block_update(100)).unwrap();
    // store_diff_only doesn't clean up, so 97 should still exist
    // (actual cleanup happens in commit_block via store_state_diff)
}

// =============================================================================
// TEST: RECOVERY WITH STORAGE SLOTS
// =============================================================================

#[tokio::test]
async fn test_recovery_preserves_storage_slots() {
    let ctx = TestContext::new("storage_recovery", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let address_hash = B256::from([0xAA; 32]);

    // Namespace 0 at block 99, crashed during write of block 102
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 102, 120);

    // Store intermediate diffs
    for block in 100..=101 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // Block 102 has account with storage
    let account_with_storage = AccountState {
        address_hash: address_hash.into(),
        balance: U256::from(1000u64),
        nonce: 1,
        code_hash: B256::ZERO,
        code: None,
        storage: {
            let mut s = HashMap::new();
            s.insert(U256::from(1), U256::from(100));
            s.insert(U256::from(2), U256::from(200));
            s
        },
        deleted: false,
    };

    let update = BlockStateUpdate {
        block_number: 102,
        block_hash: B256::from([102u8; 32]),
        state_root: B256::from([202u8; 32]),
        accounts: vec![account_with_storage],
    };
    ctx.store_diff(&mut conn, &update);

    writer.recover_stale_locks().unwrap();

    // Verify storage was recovered
    let reader = ctx.reader();
    let slot1 = reader
        .get_storage(address_hash.into(), U256::from(1), 102)
        .unwrap();
    let slot2 = reader
        .get_storage(address_hash.into(), U256::from(2), 102)
        .unwrap();

    assert_eq!(slot1, Some(U256::from(100)));
    assert_eq!(slot2, Some(U256::from(200)));
}

// =============================================================================
// TEST: BUFFER SIZE EDGE CASES
// =============================================================================

#[tokio::test]
async fn test_expected_blocks_buffer_size_5() {
    let ctx = TestContext::new("buffer_5", 5).await;
    let writer = ctx.writer();

    // With buffer_size=5 and highest_valid=100 (in ns0)
    // ns0: 100, ns1: 96, ns2: 97, ns3: 98, ns4: 99
    assert_eq!(writer.expected_block_for_namespace(0, 100), 100);
    assert_eq!(writer.expected_block_for_namespace(1, 100), 96);
    assert_eq!(writer.expected_block_for_namespace(2, 100), 97);
    assert_eq!(writer.expected_block_for_namespace(3, 100), 98);
    assert_eq!(writer.expected_block_for_namespace(4, 100), 99);
}

// =============================================================================
// TEST: FORCE RELEASE LOCK (MANUAL RECOVERY)
// =============================================================================

#[tokio::test]
async fn test_force_release_lock() {
    let ctx = TestContext::new("force_release", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    ctx.insert_stale_lock(&mut conn, 1, 100, 10); // Active lock

    // Force release (use with caution!)
    writer.force_release_lock(1).unwrap();

    assert!(!ctx.has_lock(&mut conn, 1));
}

// =============================================================================
// TEST: GET NAMESPACE BLOCK
// =============================================================================

#[tokio::test]
async fn test_get_namespace_block() {
    let ctx = TestContext::new("get_ns_block", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    ctx.set_namespace_block(&mut conn, 0, 42);
    ctx.set_namespace_block(&mut conn, 2, 100);

    assert_eq!(writer.get_namespace_block(0).unwrap(), Some(42));
    assert_eq!(writer.get_namespace_block(1).unwrap(), None);
    assert_eq!(writer.get_namespace_block(2).unwrap(), Some(100));
}

// =============================================================================
// TEST: WRITE FULL STATE TO NAMESPACE
// =============================================================================

#[tokio::test]
async fn test_write_full_state_to_namespace() {
    let ctx = TestContext::new("write_full", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let reader = ctx.reader();

    let account = AccountState {
        address_hash: B256::from([0xCC; 32]).into(),
        balance: U256::from(5000u64),
        nonce: 10,
        code_hash: B256::ZERO,
        code: None,
        storage: HashMap::new(),
        deleted: false,
    };

    let update = BlockStateUpdate {
        block_number: 50,
        block_hash: B256::from([50u8; 32]),
        state_root: B256::from([150u8; 32]),
        accounts: vec![account.clone()],
    };

    writer.write_full_state_to_namespace(2, &update).unwrap();

    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(50));

    let recovered = reader
        .get_account(account.address_hash, 50)
        .unwrap()
        .expect("Account should exist");

    assert_eq!(recovered.balance, U256::from(5000u64));
    assert_eq!(recovered.nonce, 10);
}

// =============================================================================
// TEST: RESET NAMESPACE FROM ANOTHER
// =============================================================================

#[tokio::test]
async fn test_reset_namespace_from() {
    let ctx = TestContext::new("reset_ns", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let reader = ctx.reader();

    let account = make_test_account(77);
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &[account.clone()]);

    // Reset ns0 from ns1
    writer.reset_namespace_from(0, 1).unwrap();

    // ns0 should now have ns1's data AND block number (100)
    // Note: This copies the block number too!
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(100));

    let recovered = reader
        .get_account(account.address_hash, 100) // queries ns1 since 100%3=1
        .unwrap()
        .expect("Account should exist");

    assert_eq!(recovered.balance, account.balance);
}

// =============================================================================
// TEST: RECOVERY ORDER - LOCKS BEFORE REPAIRS
// =============================================================================

#[tokio::test]
async fn test_recovery_handles_both_locks_and_corruption() {
    let ctx = TestContext::new("locks_and_corruption", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Setup: ns0 has stale lock, ns2 is corrupted
    let accounts = vec![make_test_account(1)];

    // ns0: has stale lock, needs recovery
    ctx.write_valid_namespace_state(&mut conn, 0, 99, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 100, 120);

    // ns1: valid at 100
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);

    // ns2: corrupted (wrong block)
    ctx.write_valid_namespace_state(&mut conn, 2, 50, &accounts); // Should be 98

    // Store required diffs
    for block in 98..=100 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // First recover stale locks
    let recoveries = writer.recover_stale_locks().unwrap();
    assert_eq!(recoveries.len(), 1);
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(100));

    // ns2 is still corrupted - would need repair_corrupted_namespaces
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(50));
}

// =============================================================================
// TEST: ENSURE DUMP INDEX METADATA
// =============================================================================

#[tokio::test]
async fn test_ensure_dump_index_metadata_runs_without_error() {
    let ctx = TestContext::new("dump_index", 3).await;
    let writer = ctx.writer();

    // Just verify it doesn't panic/error - we don't know the exact key format
    let result = writer.ensure_dump_index_metadata();
    assert!(result.is_ok());
}

// =============================================================================
// TEST: WRITER ID UNIQUENESS
// =============================================================================

#[tokio::test]
async fn test_writer_ids_are_unique() {
    let ctx = TestContext::new("writer_id", 3).await;

    let writer1 = ctx.writer();
    let writer2 = ctx.writer();

    assert_ne!(
        writer1.writer_id(),
        writer2.writer_id(),
        "Each writer should have unique ID"
    );
}

// =============================================================================
// TEST: BUFFER SIZE GETTER
// =============================================================================

#[tokio::test]
async fn test_buffer_size_getter() {
    let ctx = TestContext::new("buffer_size", 5).await;
    let writer = ctx.writer();

    assert_eq!(writer.buffer_size(), 5);
}

// =============================================================================
// TEST: LATEST BLOCK NUMBER
// =============================================================================

#[tokio::test]
async fn test_latest_block_number_initially_none() {
    let ctx = TestContext::new("latest_block", 3).await;
    let writer = ctx.writer();

    // Initially no blocks
    assert_eq!(writer.latest_block_number().unwrap(), None);
}

// =============================================================================
// =============================================================================
// WORKER-LEVEL SELF-HEALING TESTS
// These test the full recovery flow with trace provider integration
// =============================================================================
// =============================================================================

use std::sync::{
    Arc,
    Mutex as StdMutex,
    atomic::{
        AtomicU64,
        Ordering,
    },
};
use testcontainers::ContainerAsync;

/// Mock trace provider for testing recovery flows
struct MockTraceProvider {
    blocks: StdMutex<HashMap<u64, BlockStateUpdate>>,
    fetch_count: AtomicU64,
    fetch_history: StdMutex<Vec<u64>>,
    fail_blocks: StdMutex<HashMap<u64, usize>>, // block -> fail count
}

impl MockTraceProvider {
    fn new() -> Self {
        Self {
            blocks: StdMutex::new(HashMap::new()),
            fetch_count: AtomicU64::new(0),
            fetch_history: StdMutex::new(Vec::new()),
            fail_blocks: StdMutex::new(HashMap::new()),
        }
    }

    fn add_block(&self, update: BlockStateUpdate) {
        self.blocks
            .lock()
            .unwrap()
            .insert(update.block_number, update);
    }

    fn get_fetch_history(&self) -> Vec<u64> {
        self.fetch_history.lock().unwrap().clone()
    }

    fn get_fetch_count(&self) -> u64 {
        self.fetch_count.load(Ordering::SeqCst)
    }

    /// Make a block fail N times before succeeding
    fn fail_block_n_times(&self, block: u64, times: usize) {
        self.fail_blocks.lock().unwrap().insert(block, times);
    }
}

/// Trait that mirrors the real TraceProvider for testing
#[async_trait]
trait TraceProvider: Send + Sync {
    async fn fetch_block_state(&self, block_number: u64) -> anyhow::Result<BlockStateUpdate>;
}

#[async_trait]
impl TraceProvider for MockTraceProvider {
    async fn fetch_block_state(&self, block_number: u64) -> anyhow::Result<BlockStateUpdate> {
        self.fetch_count.fetch_add(1, Ordering::SeqCst);
        self.fetch_history.lock().unwrap().push(block_number);

        // Check if this block should fail
        let mut fail_blocks = self.fail_blocks.lock().unwrap();
        if let Some(remaining) = fail_blocks.get_mut(&block_number)
            && *remaining > 0
        {
            *remaining -= 1;
            return Err(anyhow!("Simulated failure for block {block_number}"));
        }
        drop(fail_blocks);

        self.blocks
            .lock()
            .unwrap()
            .get(&block_number)
            .cloned()
            .ok_or_else(|| anyhow!("block {} not found in mock", block_number))
    }
}

// =============================================================================
// TEST: WORKER RECOVERY - FETCHES MISSING DIFF FROM PROVIDER
// =============================================================================

/// This test simulates the full worker recovery flow:
/// 1. Writer.recover_stale_locks() returns MissingStateDiff
/// 2. Worker calls trace_provider.fetch_block_state()
/// 3. Worker stores the diff
/// 4. Worker retries recovery and succeeds
#[tokio::test]
async fn test_worker_recovery_fetches_missing_diff() {
    let ctx = TestContext::new("worker_fetch_diff", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Setup: namespace 1 already at block 97, stale lock for block 100
    // Recovery will need diffs 98, 99, 100
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Add blocks 98, 99, 100 to the provider
    for block in 98..=100 {
        provider.add_block(make_block_update(block));
    }

    // Simulate worker recovery loop
    let max_attempts = 10;
    let mut attempt = 0;
    let mut success = false;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(recoveries) => {
                assert_eq!(recoveries.len(), 1);
                success = true;
                break;
            }
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                // Fetch from provider and store (this is what the worker does)
                let update = provider.fetch_block_state(needed_block).await.unwrap();
                writer.store_diff_only(&update).unwrap();
                // Loop will retry
            }
            Err(e) => panic!("Unexpected error: {e:?}"),
        }
    }

    assert!(success, "Recovery should succeed after fetching diffs");
    assert_eq!(
        provider.get_fetch_count(),
        3,
        "Should fetch 3 diffs (98, 99, 100)"
    );
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
}

// =============================================================================
// TEST: WORKER RECOVERY - FETCHES MULTIPLE MISSING DIFFS
// =============================================================================

#[tokio::test]
async fn test_worker_recovery_fetches_multiple_diffs() {
    let ctx = TestContext::new("worker_multi_fetch", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Setup: namespace at 97, lock for 100
    // Need diffs 98, 99, 100
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Add all blocks to provider
    for block in 98..=100 {
        provider.add_block(make_block_update(block));
    }

    // Simulate worker recovery loop
    let max_attempts = 10;
    let mut attempt = 0;
    let mut success = false;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(recoveries) => {
                assert_eq!(recoveries.len(), 1);
                success = true;
                break;
            }
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                let update = provider.fetch_block_state(needed_block).await.unwrap();
                writer.store_diff_only(&update).unwrap();
            }
            Err(e) => panic!("Unexpected error: {e:?}"),
        }
    }

    assert!(success, "Recovery should succeed");
    assert_eq!(provider.get_fetch_history(), vec![98, 99, 100]);
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
}

#[tokio::test]
async fn test_worker_recovery_retries_on_provider_failure() {
    let ctx = TestContext::new("worker_retry_provider", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Setup: namespace 0 at block 47, stale lock for block 50
    // (50 % 3 = 2, but we want ns 0, so let's use block 48 which is 48 % 3 = 0)
    // Actually, let's use namespace 2: 50 % 3 = 2
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 2, 47, &accounts);
    ctx.insert_stale_lock(&mut conn, 2, 50, 120);

    // Add blocks 48, 49, 50 to provider
    for block in 48..=50 {
        provider.add_block(make_block_update(block));
    }

    // Make block 48 fail twice before succeeding
    provider.fail_block_n_times(48, 2);

    let max_attempts = 15;
    let mut attempt = 0;
    let mut success = false;
    let mut provider_failures = 0;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(_) => {
                success = true;
                break;
            }
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                match provider.fetch_block_state(needed_block).await {
                    Ok(update) => {
                        writer.store_diff_only(&update).unwrap();
                    }
                    Err(_) => {
                        provider_failures += 1;
                        // Don't store anything, will retry on next iteration
                    }
                }
            }
            Err(e) => panic!("Unexpected error: {e:?}"),
        }
    }

    assert!(success, "Recovery should eventually succeed");
    assert_eq!(
        provider_failures, 2,
        "Provider should fail twice on block 48"
    );
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(50));
}

// =============================================================================
// TEST: WORKER RECOVERY - FAILS AFTER MAX ATTEMPTS
// =============================================================================

#[tokio::test]
async fn test_worker_recovery_fails_after_max_attempts() {
    let ctx = TestContext::new("worker_max_attempts", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    ctx.insert_stale_lock(&mut conn, 0, 50, 120);
    // Provider does NOT have block 50

    let max_attempts = 8;
    let mut attempt = 0;
    let mut last_error = None;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(_) => break,
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                match provider.fetch_block_state(needed_block).await {
                    Ok(update) => {
                        writer.store_diff_only(&update).unwrap();
                    }
                    Err(e) => {
                        last_error = Some(e);
                        if attempt == max_attempts {
                            break;
                        }
                    }
                }
            }
            Err(e) => {
                last_error = Some(anyhow::anyhow!("{e}"));
                break;
            }
        }
    }

    assert!(last_error.is_some(), "Should fail with error");
    assert_eq!(attempt, max_attempts);
    assert!(ctx.has_lock(&mut conn, 0), "Lock should remain on failure");
}

// =============================================================================
// TEST: WORKER NAMESPACE REPAIR - CORRECT BLOCK PER NAMESPACE
// =============================================================================

/// This test verifies the critical fix: each namespace gets its EXPECTED block,
/// not the valid namespace's block.
#[tokio::test]
async fn test_worker_repair_fetches_correct_block_per_namespace() {
    let ctx = TestContext::new("worker_correct_blocks", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Setup: ns1 is valid at 100, ns0 and ns2 need repair
    // Expected: ns0=99, ns1=100, ns2=98
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 50, &accounts); // WRONG
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts); // VALID
    ctx.write_valid_namespace_state(&mut conn, 2, 60, &accounts); // WRONG

    // Add needed blocks to provider
    for block in 98..=100 {
        provider.add_block(make_block_update(block));
    }

    // Verify expected block calculations
    assert_eq!(writer.expected_block_for_namespace(0, 100), 99);
    assert_eq!(writer.expected_block_for_namespace(1, 100), 100);
    assert_eq!(writer.expected_block_for_namespace(2, 100), 98);

    // Simulate repair flow
    let valid = writer.find_valid_namespace_state().unwrap().unwrap();
    assert_eq!(valid, (1, 100));

    // Repair would need to:
    // - For ns0 (expected=99): fetch 99, 100 diffs, copy from ns1, apply
    // - For ns2 (expected=98): fetch 98, 99, 100 diffs, copy from ns1, apply

    // Store diffs for repair
    for block in 98..=100 {
        if !writer.has_diff(block).unwrap() {
            let update = provider.fetch_block_state(block).await.unwrap();
            writer.store_diff_only(&update).unwrap();
        }
    }

    // The key insight: we need to use a source namespace with block BEFORE target
    // ns1 is at 100, but we need to repair ns2 to 98 - can't use ns1!
    // This is why repair_namespace_from_valid_state checks source < target

    // In practice, the worker would need to find an older valid state or
    // write the full state directly. Let's test write_full_state_to_namespace:

    let update_98 = provider.fetch_block_state(98).await.unwrap();
    let update_99 = provider.fetch_block_state(99).await.unwrap();

    writer.write_full_state_to_namespace(2, &update_98).unwrap();
    writer.write_full_state_to_namespace(0, &update_99).unwrap();

    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(99));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(98));

    // Verify fetch history - should have fetched 98, 99, 100
    let history = provider.get_fetch_history();
    assert!(history.contains(&98));
    assert!(history.contains(&99));
    assert!(history.contains(&100));
}

// =============================================================================
// TEST: WORKER REPAIR - DOES NOT COPY WRONG BLOCK NUMBER
// =============================================================================

/// Critical test: repair must NOT copy the source's block number to target
#[tokio::test]
async fn test_worker_repair_does_not_copy_block_number() {
    let ctx = TestContext::new("no_copy_block", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // ns1 at 97, repair ns2 to 99
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);

    // Add diffs
    for block in 98..=99 {
        provider.add_block(make_block_update(block));
        let update = provider.fetch_block_state(block).await.unwrap();
        writer.store_diff_only(&update).unwrap();
    }

    // Repair ns2 from ns1 to reach block 99
    writer.repair_namespace_from_valid_state(2, 1, 99).unwrap();

    // CRITICAL: ns2 should be at 99, NOT 97
    let ns2_block = ctx.get_namespace_block(&mut conn, 2).unwrap();
    let ns1_block = ctx.get_namespace_block(&mut conn, 1).unwrap();

    assert_eq!(ns2_block, 99, "ns2 should be at target block 99");
    assert_eq!(ns1_block, 97, "ns1 should remain at 97");
    assert_ne!(ns2_block, ns1_block, "ns2 must NOT copy ns1's block number");
}

// =============================================================================
// TEST: RECOVERY FLOW - COMBINED SCENARIO
// =============================================================================

/// Full integration scenario combining stale locks and namespace corruption
#[tokio::test]
async fn test_full_recovery_scenario() {
    let ctx = TestContext::new("full_scenario", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    // Setup complex scenario:
    // - ns0: stale lock, crashed during write of block 300
    // - ns1: valid at 298
    // - ns2: corrupted, has block 50 but should have 299

    let accounts = vec![make_test_account(1)];

    ctx.write_valid_namespace_state(&mut conn, 0, 297, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 300, 120);

    ctx.write_valid_namespace_state(&mut conn, 1, 298, &accounts);

    ctx.write_valid_namespace_state(&mut conn, 2, 50, &accounts); // WRONG

    // Add all needed blocks to provider
    for block in 298..=300 {
        provider.add_block(make_block_update(block));
    }

    // Phase 1: Recover stale lock
    let max_attempts = 10;
    let mut attempt = 0;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(recoveries) => {
                assert_eq!(recoveries.len(), 1);
                assert_eq!(recoveries[0].target_block, 300);
                break;
            }
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                let update = provider.fetch_block_state(needed_block).await.unwrap();
                writer.store_diff_only(&update).unwrap();
            }
            Err(e) => panic!("Unexpected error: {e:?}"),
        }
    }

    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(300));
    assert!(!ctx.has_lock(&mut conn, 0));

    // Phase 2: Repair corrupted namespace
    // Now highest valid is 300 (ns0)
    // Expected: ns0=300, ns1=298, ns2=299

    let valid = writer.find_valid_namespace_state().unwrap().unwrap();
    assert_eq!(valid.0, 0); // ns0 is now valid with highest block
    assert_eq!(valid.1, 300);

    // ns2 needs to be at 299, repair from ns1 (at 298)
    writer.repair_namespace_from_valid_state(2, 1, 299).unwrap();

    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(299));

    // Final state verification
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(300));
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(298));
    assert_eq!(ctx.get_namespace_block(&mut conn, 2), Some(299));
}

// =============================================================================
// TEST: BLOCK 6844336 - PRODUCTION SCENARIO
// =============================================================================

/// Test the exact scenario from production error
#[tokio::test]
async fn test_block_6844336_production_scenario() {
    let ctx = TestContext::new("prod_6844336", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();
    let provider = Arc::new(MockTraceProvider::new());

    let block = 6844336u64;
    let ns_idx = (block % 3) as usize;
    assert_eq!(ns_idx, 1, "Block 6844336 should be in namespace 1");

    // Setup: crash during write of block 6844336
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 6844333, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, block, 120);

    // Need diffs 6844334, 6844335, 6844336
    for b in 6844334..=block {
        provider.add_block(make_block_update(b));
    }

    // Recovery loop
    let max_attempts = 10;
    let mut attempt = 0;

    while attempt < max_attempts {
        attempt += 1;
        match writer.recover_stale_locks() {
            Ok(recoveries) => {
                assert_eq!(recoveries.len(), 1);
                assert_eq!(recoveries[0].target_block, block);
                break;
            }
            Err(state_store::common::error::StateError::MissingStateDiff {
                needed_block, ..
            }) => {
                let update = provider.fetch_block_state(needed_block).await.unwrap();
                writer.store_diff_only(&update).unwrap();
            }
            Err(e) => panic!("Unexpected error: {e:?}"),
        }
    }

    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(block));
    assert!(!ctx.has_lock(&mut conn, 1));

    // Verify expected circular buffer layout at this block
    let expected_ns0 = block - 1; // 6844335
    let expected_ns1 = block; // 6844336
    let expected_ns2 = block - 2; // 6844334

    assert_eq!(expected_ns0 % 3, 0);
    assert_eq!(expected_ns1 % 3, 1);
    assert_eq!(expected_ns2 % 3, 2);
}

// =============================================================================
// STATE INTEGRITY GUARANTEE TESTS
// These verify the CRITICAL invariant: state is NEVER left corrupted
// =============================================================================

/// CRITICAL: Partial diff application must not corrupt state
/// If we can't apply all diffs, we must not apply any
#[tokio::test]
async fn test_no_partial_diff_application() {
    let ctx = TestContext::new("no_partial", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // ns1 at 95, lock for 100 - need 5 diffs
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 95, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Only store diffs 96 and 97 (missing 98, 99, 100)
    ctx.store_diff(&mut conn, &make_block_update(96));
    ctx.store_diff(&mut conn, &make_block_update(97));

    let result = writer.recover_stale_locks();
    assert!(result.is_err());

    // CRITICAL: Namespace must still be at 95, NOT 97
    // We must not have applied partial diffs
    let block = ctx.get_namespace_block(&mut conn, 1).unwrap();
    assert_eq!(
        block, 95,
        "Must not apply partial diffs - state would be corrupted"
    );
}

/// CRITICAL: Failed repair must not modify target namespace
#[tokio::test]
async fn test_failed_repair_no_side_effects() {
    let ctx = TestContext::new("no_side_effects", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Source at 90, try to repair target to 100 (missing diffs)
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 80, &accounts); // Target
    ctx.write_valid_namespace_state(&mut conn, 1, 90, &accounts); // Source

    // Only store diff 91, missing 92-100
    ctx.store_diff(&mut conn, &make_block_update(91));

    let result = writer.repair_namespace_from_valid_state(0, 1, 100);
    assert!(result.is_err());

    // Target namespace must be UNCHANGED
    let block = ctx.get_namespace_block(&mut conn, 0).unwrap();
    assert_eq!(block, 80, "Failed repair must not modify target");
}

/// CRITICAL: Lock must NEVER be released if state is inconsistent
#[tokio::test]
async fn test_lock_never_released_on_inconsistent_state() {
    let ctx = TestContext::new("lock_protection", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Create a lock where recovery is impossible (no diffs available)
    ctx.insert_stale_lock(&mut conn, 2, 999, 120);

    // Multiple recovery attempts should all fail
    for _ in 0..5 {
        let result = writer.recover_stale_locks();
        assert!(result.is_err());
    }

    // Lock MUST still be in place protecting readers
    assert!(
        ctx.has_lock(&mut conn, 2),
        "Lock is the last line of defense!"
    );
}

/// CRITICAL: Recovery must be idempotent
#[tokio::test]
async fn test_recovery_is_idempotent() {
    let ctx = TestContext::new("idempotent", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 100, &accounts);
    ctx.insert_stale_lock(&mut conn, 0, 100, 120);

    // Run recovery multiple times
    for i in 0..3 {
        let recoveries = writer.recover_stale_locks().unwrap();
        if i == 0 {
            assert_eq!(recoveries.len(), 1, "First run should recover");
        } else {
            assert!(recoveries.is_empty(), "Subsequent runs should be no-ops");
        }
    }

    // State should be valid and consistent
    assert_eq!(ctx.get_namespace_block(&mut conn, 0), Some(100));
    assert!(!ctx.has_lock(&mut conn, 0));
}

/// CRITICAL: Concurrent recovery attempts should be safe
#[tokio::test]
async fn test_concurrent_recovery_safety() {
    let ctx = TestContext::new("concurrent", 3).await;
    let mut conn = ctx.conn();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 100, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Create two writers (simulating concurrent recovery attempts)
    let writer1 = ctx.writer();
    let writer2 = ctx.writer();

    // Both attempt recovery
    let result1 = writer1.recover_stale_locks();
    let result2 = writer2.recover_stale_locks();

    // At least one should succeed, and state should be consistent
    let success_count = result1.is_ok() as i32 + result2.is_ok() as i32;
    assert!(success_count >= 1, "At least one should succeed");

    // Final state must be consistent
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
    assert!(!ctx.has_lock(&mut conn, 1));
}

// =============================================================================
// CIRCULAR BUFFER BOUNDARY TESTS
// =============================================================================

/// Test behavior at block 0 (genesis edge case)
#[tokio::test]
async fn test_circular_buffer_block_zero() {
    let ctx = TestContext::new("block_zero", 3).await;
    let writer = ctx.writer();

    // At block 0, all expected blocks should be 0
    assert_eq!(writer.expected_block_for_namespace(0, 0), 0);
    // For blocks 1 and 2, the calculation with block 0 as highest might give odd results
    // depending on implementation - let's verify the actual behavior
}

/// Test behavior when only one block has been processed
#[tokio::test]
async fn test_circular_buffer_single_block() {
    let ctx = TestContext::new("single_block", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 0, 0, &accounts);

    let valid = writer.find_valid_namespace_state().unwrap();
    assert_eq!(valid, Some((0, 0)));
}

/// Test transition across buffer size boundary
#[tokio::test]
async fn test_circular_buffer_boundary_transition() {
    let ctx = TestContext::new("boundary", 3).await;
    let writer = ctx.writer();

    // At blocks 2, 3, 4 (transitioning from initial fill to wrap)
    // Block 2: ns0=0, ns1=1, ns2=2
    assert_eq!(writer.expected_block_for_namespace(0, 2), 0);
    assert_eq!(writer.expected_block_for_namespace(1, 2), 1);
    assert_eq!(writer.expected_block_for_namespace(2, 2), 2);

    // Block 3: ns0=3, ns1=1, ns2=2 (first wrap!)
    assert_eq!(writer.expected_block_for_namespace(0, 3), 3);
    assert_eq!(writer.expected_block_for_namespace(1, 3), 1);
    assert_eq!(writer.expected_block_for_namespace(2, 3), 2);

    // Block 4: ns0=3, ns1=4, ns2=2
    assert_eq!(writer.expected_block_for_namespace(0, 4), 3);
    assert_eq!(writer.expected_block_for_namespace(1, 4), 4);
    assert_eq!(writer.expected_block_for_namespace(2, 4), 2);
}

// =============================================================================
// ERROR MESSAGE CLARITY TESTS
// =============================================================================

#[tokio::test]
async fn test_error_messages_contain_useful_info() {
    let ctx = TestContext::new("error_msgs", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Test MissingStateDiff error
    ctx.insert_stale_lock(&mut conn, 0, 100, 120);
    let result = writer.recover_stale_locks();
    let err = result.unwrap_err();
    let msg = format!("{err}");
    assert!(
        msg.contains("100") || msg.contains("block"),
        "Error should mention target block"
    );

    // Test InvalidNamespace error
    let result = writer.force_recover_namespace(99);
    let err = result.unwrap_err();
    let msg = format!("{err}");
    assert!(
        msg.contains("99") || msg.contains("namespace"),
        "Error should mention namespace"
    );
}

// =============================================================================
// DIFF STORAGE TESTS
// =============================================================================

#[tokio::test]
async fn test_diff_contains_all_fields() {
    let ctx = TestContext::new("diff_fields", 3).await;
    let mut conn = ctx.conn();

    let update = BlockStateUpdate {
        block_number: 42,
        block_hash: B256::from([0xAB; 32]),
        state_root: B256::from([0xCD; 32]),
        accounts: vec![AccountState {
            address_hash: B256::from([0x11; 32]).into(),
            balance: U256::from(1000u64),
            nonce: 5,
            code_hash: B256::from([0x22; 32]),
            code: Some(vec![0x60, 0x80]),
            storage: {
                let mut s = HashMap::new();
                s.insert(U256::from(1), U256::from(100));
                s
            },
            deleted: false,
        }],
    };

    ctx.store_diff(&mut conn, &update);

    // Read it back
    let diff_key = format!("{}:diff:42", ctx.base_namespace);
    let json: String = redis::cmd("GET").arg(&diff_key).query(&mut conn).unwrap();
    let recovered: BlockStateUpdate = serde_json::from_str(&json).unwrap();

    assert_eq!(recovered.block_number, 42);
    assert_eq!(recovered.block_hash, B256::from([0xAB; 32]));
    assert_eq!(recovered.state_root, B256::from([0xCD; 32]));
    assert_eq!(recovered.accounts.len(), 1);
    assert_eq!(recovered.accounts[0].nonce, 5);
    assert!(recovered.accounts[0].code.is_some());
    assert!(!recovered.accounts[0].storage.is_empty());
}

// =============================================================================
// STRESS TESTS
// =============================================================================

#[tokio::test]
async fn test_recovery_with_many_accounts() {
    let ctx = TestContext::new("many_accounts", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    // Namespace 1 at block 97, crashed during write of block 100
    // (100 % 3 = 1)
    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 97, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 100, 120);

    // Store intermediate diffs
    for block in 98..=99 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    // Create diff with 1000 accounts for block 100
    let many_accounts: Vec<AccountState> = (0..1000)
        .map(|i| {
            AccountState {
                address_hash: B256::from([(i % 256) as u8; 32]).into(),
                balance: U256::from(i as u64),
                nonce: i as u64,
                code_hash: B256::ZERO,
                code: None,
                storage: HashMap::new(),
                deleted: false,
            }
        })
        .collect();

    let update = BlockStateUpdate {
        block_number: 100,
        block_hash: B256::from([100u8; 32]),
        state_root: B256::from([200u8; 32]),
        accounts: many_accounts,
    };
    ctx.store_diff(&mut conn, &update);

    let recoveries = writer.recover_stale_locks().unwrap();
    assert_eq!(recoveries.len(), 1);
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(100));
}

#[tokio::test]
async fn test_recovery_chain_of_many_diffs() {
    let ctx = TestContext::new("many_diffs", 3).await;
    let mut conn = ctx.conn();
    let writer = ctx.writer();

    let accounts = vec![make_test_account(1)];
    ctx.write_valid_namespace_state(&mut conn, 1, 0, &accounts);
    ctx.insert_stale_lock(&mut conn, 1, 50, 120); // Need 50 diffs!

    for block in 1..=50 {
        ctx.store_diff(&mut conn, &make_block_update(block));
    }

    let recoveries = writer.recover_stale_locks().unwrap();
    assert_eq!(recoveries.len(), 1);
    assert_eq!(ctx.get_namespace_block(&mut conn, 1), Some(50));
}
